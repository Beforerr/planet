[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Planets? Yes!",
    "section": "",
    "text": "pip install ids-finder"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Planets? Yes!",
    "section": "",
    "text": "pip install ids-finder"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Planets? Yes!",
    "section": "How to use",
    "text": "How to use\nImport the package\n\nfrom ids_finder.core import *\nfrom ids_finder.utils import *"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n calc_vec_mag (vec)\n\n\nsource\n\n\n\n\n juno_get_state (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fra\n                 me.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\n\n\n sat_get_fgm_from_df (df:Union[pandas.core.frame.DataFrame,polars.datafram\n                      e.frame.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\n\n\n df2ts (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n        ame,polars.lazyframe.frame.LazyFrame], cols, attrs, name=None)\n\n\nsource\n\n\n\n\n col_renamer (lbl:str)\n\n\nsource\n\n\n\n\n check_fgm (vec)\n\n\nsource\n\n\n\n\n download_file (url, local_dir='./', file_name=None)\n\nDownload a file from a URL and save it locally.\nReturns: file_path (str): Path to the downloaded file.\n\n\n\n\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame\n\n\n\n\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame\n\nsource\n\n\n\n\n pl_dvec (columns, *more_columns)\n\n\nsource\n\n\n\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nsource\n\n\n\n\n pl_format_time (tau)\n\n\nsource\n\n\n\n\n calc_vec_relative_diff (vec:xarray.core.dataarray.DataArray)\n\nComputes the relative difference between the last and first elements of a vector.\n\nsource\n\n\n\n\n calc_vec_std (vec:xarray.core.dataarray.DataArray)\n\nComputes the standard deviation of a vector.\n\nsource\n\n\n\n\n calc_vec_mean_mag (vec:xarray.core.dataarray.DataArray)\n\n\nsource\n\n\n\n\n compute_combinded_std (df:polars.dataframe.frame.DataFrame, tau)\n\n\nsource\n\n\n\n\n compute_std (df:polars.dataframe.frame.DataFrame, tau)\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;LazyFrame, object&gt; \n---------------------------- in \nMultiply dispatched method: compute_index_std\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;lazyframe, Object&gt;\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\n\n\n\n\n compute_index_std (*args, **kwargs)\n\nMultiply dispatched method: compute_index_std\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;LazyFrame, timedelta&gt; \n------------------------------- in \nMultiply dispatched method: compute_indices\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;DataFrame, timedelta&gt; \n------------------------------- in \nMultiply dispatched method: compute_indices\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;lazyframe, Timedelta&gt;\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;dataframe, Timedelta&gt;\n  else: warn(msg)\n\n\n\n\n\n compute_indices (*args, **kwargs)\n\nMultiply dispatched method: compute_indices\n\n\n\n\n\n compute_indices (*args, **kwargs)\n\nMultiply dispatched method: compute_indices\n\nsource\n\n\n\n\n compute_index_diff (df, tau)"
  },
  {
    "objectID": "utils.html#utilities-functions",
    "href": "utils.html#utilities-functions",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n calc_vec_mag (vec)\n\n\nsource\n\n\n\n\n juno_get_state (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fra\n                 me.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\n\n\n sat_get_fgm_from_df (df:Union[pandas.core.frame.DataFrame,polars.datafram\n                      e.frame.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\n\n\n df2ts (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n        ame,polars.lazyframe.frame.LazyFrame], cols, attrs, name=None)\n\n\nsource\n\n\n\n\n col_renamer (lbl:str)\n\n\nsource\n\n\n\n\n check_fgm (vec)\n\n\nsource\n\n\n\n\n download_file (url, local_dir='./', file_name=None)\n\nDownload a file from a URL and save it locally.\nReturns: file_path (str): Path to the downloaded file.\n\n\n\n\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame\n\n\n\n\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame\n\nsource\n\n\n\n\n pl_dvec (columns, *more_columns)\n\n\nsource\n\n\n\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nsource\n\n\n\n\n pl_format_time (tau)\n\n\nsource\n\n\n\n\n calc_vec_relative_diff (vec:xarray.core.dataarray.DataArray)\n\nComputes the relative difference between the last and first elements of a vector.\n\nsource\n\n\n\n\n calc_vec_std (vec:xarray.core.dataarray.DataArray)\n\nComputes the standard deviation of a vector.\n\nsource\n\n\n\n\n calc_vec_mean_mag (vec:xarray.core.dataarray.DataArray)\n\n\nsource\n\n\n\n\n compute_combinded_std (df:polars.dataframe.frame.DataFrame, tau)\n\n\nsource\n\n\n\n\n compute_std (df:polars.dataframe.frame.DataFrame, tau)\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;LazyFrame, object&gt; \n---------------------------- in \nMultiply dispatched method: compute_index_std\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;lazyframe, Object&gt;\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\n\n\n\n\n compute_index_std (*args, **kwargs)\n\nMultiply dispatched method: compute_index_std\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;LazyFrame, timedelta&gt; \n------------------------------- in \nMultiply dispatched method: compute_indices\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs: &lt;DataFrame, timedelta&gt; \n------------------------------- in \nMultiply dispatched method: compute_indices\n...\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;lazyframe, Timedelta&gt;\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Inputs: &lt;dataframe, Timedelta&gt;\n  else: warn(msg)\n\n\n\n\n\n compute_indices (*args, **kwargs)\n\nMultiply dispatched method: compute_indices\n\n\n\n\n\n compute_indices (*args, **kwargs)\n\nMultiply dispatched method: compute_indices\n\nsource\n\n\n\n\n compute_index_diff (df, tau)"
  },
  {
    "objectID": "utils.html#obsolete-codes",
    "href": "utils.html#obsolete-codes",
    "title": "Utils",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\nfrom flox.xarray import xarray_reduce\n\n\n@dispatch(xr.DataArray, object)\ndef compute_index_std(data: DataArray, tau):\n    \"\"\"\n    Examples\n    --------\n    &gt;&gt;&gt; i1 = index_std(juno_fgm_b, tau)\n    \"\"\"\n\n    # NOTE: large tau values will speed up the computation\n\n    # Resample the data based on the provided time interval.\n    grouped_data = data.resample(time=pandas.Timedelta(tau, unit=\"s\"))\n\n    # Compute the standard deviation for all groups\n    vec_stds = linalg.norm(grouped_data.std(dim=\"time\"), dims=\"v_dim\")\n    # vec_stds = grouped_data.map(calc_vec_std) # NOTE: This is way much slower (like 30x slower)\n\n    offset = pandas.Timedelta(tau / 2, unit=\"s\")\n    vec_stds[\"time\"] = vec_stds[\"time\"] + offset\n\n    vec_stds_next = vec_stds.assign_coords(\n        {\"time\": vec_stds[\"time\"] - pandas.Timedelta(tau, unit=\"s\")}\n    )\n    vec_stds_previous = vec_stds.assign_coords(\n        {\"time\": vec_stds[\"time\"] + pandas.Timedelta(tau, unit=\"s\")}\n    )\n    return np.minimum(vec_stds / vec_stds_next, vec_stds / vec_stds_previous)\n\n\ndef index_diff(data: DataArray, tau):\n    grouped_data = data.resample(time=pandas.Timedelta(tau, unit='s'))\n\n    dvecs = grouped_data.first()-grouped_data.last()\n    vec_mean_mags = grouped_data.map(calc_vec_mean_mag)\n    vec_diffs = linalg.norm(dvecs, dims='v_dim') / vec_mean_mags\n    \n    # vec_diffs = grouped_data.map(calc_vec_relative_diff) # NOTE: this is slower than the above implementation.\n    # INFO: Do your spatial and temporal indexing (e.g. .sel() or .isel()) early in the pipeline, especially before calling resample() or groupby(). Grouping and resampling triggers some computation on all the blocks, which in theory should commute with indexing, but this optimization hasn’t been implemented in Dask yet. (See Dask issue #746).\n    \n    offset = pandas.Timedelta(tau/2, unit='s')\n    vec_diffs['time'] = vec_diffs['time'] + offset\n    return vec_diffs\n\n\n# %%\ndef _compute_indices_old(df: pl.DataFrame, tau):\n    \"\"\"\n    Compute all index based on the given DataFrame and tau value.\n\n    Parameters\n    ----------\n    - df (pl.DataFrame): The input DataFrame.\n    - tau (int): The time interval value.\n\n    Returns\n    -------\n    - tuple: Tuple containing DataFrame results for fluctuation index, standard deviation index, and 'index_num'.\n\n    Examples\n    --------\n    &gt;&gt;&gt; indices = compute_indices(df, tau)\n\n    Notes\n    -----\n    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n    \"\"\"\n\n    if isinstance(tau, (int, float)):\n        tau = timedelta(seconds=tau)\n\n    b_cols = [\"BX\", \"BY\", \"BZ\"]\n    db_cols = [\"d\" + col_name + \"_vec\" for col_name in b_cols]\n\n    std_next = pl.col(\"B_std\").shift(-2)\n    std_current = pl.col(\"B_std\")\n    std_previous = pl.col(\"B_std\").shift(2)\n    pl_std_index = pl.min_horizontal(\n        [std_current / std_previous, std_current / std_next]\n    )\n\n    # Compute fluctuation index\n    group_df = (\n        df.with_columns(pl_norm(\"BX\", \"BY\", \"BZ\").alias(\"B\"))\n        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n        .agg(\n            pl.count(),\n            pl.col(b_cols),\n            pl.col(\"BX\").std().alias(\"BX_std\"),\n            pl.col(\"BY\").std().alias(\"BY_std\"),\n            pl.col(\"BZ\").std().alias(\"BZ_std\"),\n            pl.col(\"B\").mean().alias(\"B_mean\"),\n            *pl_dvec(b_cols),\n        )\n        .with_columns(\n            pl_norm(\"BX_std\", \"BY_std\", \"BZ_std\").alias(\"B_std\"),\n            pl_norm(db_cols).alias(\"dB_vec\"),\n        )\n        .drop(\"BX_std\", \"BY_std\", \"BZ_std\")\n    )\n\n    indices = (\n        group_df.with_columns(\n            calc_combined_std(\"BX\"),\n            calc_combined_std(\"BY\"),\n            calc_combined_std(\"BZ\"),\n        )\n        .drop(\"BX\", \"BY\", \"BZ\")\n        .with_columns(\n            pl_norm(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\").alias(\n                \"B_combined_std\"\n            ),\n            pl.sum_horizontal(\n                pl.col(\"B_std\").shift(-2), pl.col(\"B_std\").shift(2)\n            ).alias(\"B_added_std\"),\n        )\n        .drop(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\")\n        .with_columns(\n            pl_std_index.alias(\"index_std\"),\n            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n                \"index_fluctuation\"\n            ),\n            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n        )\n    )\n    return indices"
  },
  {
    "objectID": "stereo.html",
    "href": "stereo.html",
    "title": "IDs from STEREO",
    "section": "",
    "text": "import all the packages needed for the project\nfrom ids_finder.utils import *\nfrom ids_finder.core import *\nfrom fastcore.utils import *\nfrom fastcore.test import *\n\nimport polars as pl\ntry:\n    import modin.pandas as pd\n    import modin.pandas as mpd\nexcept ImportError:\n    import pandas as pd\n\nimport pandas\nimport numpy as np\nfrom humanize import naturalsize\nimport xarray as xr\n\n\nfrom datetime import timedelta\nfrom loguru import logger\nimport speasy as spz\nfrom multipledispatch import dispatch\n\nimport altair as alt\nCode\nfrom speasy.products import SpeasyVariable\nCode\ncda_tree: spz.SpeasyIndex = spz.inventories.tree.cda\nproduct = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN\n\nlogger.info(product.description)\nlogger.info(product.ID)\nlogger.info(product.BFIELD.CATDESC)\nlogger.info(product.BFIELD.spz_uid())\n\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.\n# spz.inventories.data_tree.cda.STEREO.STEREOA.IMPACT_MAG.STA_LB_MAG_RTN.description\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.MAGFLAGUC.CATDESC\nspz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD.CATDESC\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD.\n\n\n2023-09-28 10:51:32.593 | INFO     | __main__:&lt;cell line: 4&gt;:4 - STEREO Ahead IMPACT/MAG Magnetic Field Vectors (RTN) - J. Luhmann (UCB/SSL)\n2023-09-28 10:51:32.593 | INFO     | __main__:&lt;cell line: 5&gt;:5 - sta_l1_mag_rtn_cdaweb\n2023-09-28 10:51:32.594 | INFO     | __main__:&lt;cell line: 6&gt;:6 - Magnetic field vector in RTN coordinates from the IMPACT/MAG instrument.\n2023-09-28 10:51:32.594 | INFO     | __main__:&lt;cell line: 7&gt;:7 - STA_L1_MAG_RTN/BFIELD\n\n\n'Magnetic field vector in RTN coordinates from the IMPACT/MAG instrument.'\nCode\ntype(product)\n\n\nspeasy.core.inventory.indexes.DatasetIndex\nCode\ndef data_preview(data: SpeasyVariable):\n    print(\"===========================================\")\n    print(f\"Name:         {data.name}\")\n    print(f\"Columns:      {data.columns}\")\n    print(f\"Values Unit:  {data.unit}\")\n    print(f\"Memory usage: {naturalsize(data.nbytes)}\")\n    print(f\"Axes Labels:  {data.axes_labels}\")\n    print(\"-------------------------------------------\")\n    print(f\"Meta-data:    {data.meta}\")\n    print(\"-------------------------------------------\")\n    print(f\"Time Axis:    {data.time[:3]}\")\n    print(\"-------------------------------------------\")\n    print(f\"Values:       {data.values[:3]}\")\n    print(\"===========================================\")"
  },
  {
    "objectID": "stereo.html#dataset-overview",
    "href": "stereo.html#dataset-overview",
    "title": "IDs from STEREO",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nCode\nstereo_probes = [\"a\", \"b\"]\nprobe = stereo_probes[0]\n\njno_start_date = \"2011-08-25\"\njno_end_date = \"2016-06-30\" \n\ntrange = [jno_start_date, jno_end_date]\ntest_trange = [\"2011-08-25\", \"2012-08-26\"]\n\n\n\nDownload all the files\n\n\nCode\nsat = 'STA'\ncoord = 'rtn'\n\nsat_fgm_product = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD\nsat_fgm_product = 'cda/STA_L1_MAG_RTN/BFIELD'\nproducts = [sat_fgm_product]\n\n\n\n\nCode\n@threaded\ndef download_data(products, trange):\n    logger.info(\"Downloading data\")\n    spz.get_data(products, trange, disable_proxy=True)\n    logger.info(\"Data downloaded\")\n    # spz.get_data(products, jno_start_date, jno_end_date)\n\n\nDownload data in a background thread\n\n\nCode\n# %%markdown\n#| eval: false\n\n# download_data(products, trange)\n\n\n\n\nCode\ndataset = spz.get_data(products, test_trange, disable_proxy=True)\nsat_fgm_data: SpeasyVariable  = dataset[0]\ndata_preview(sat_fgm_data)\n\n\n28-Sep-23 10:53:55: Using pycdfpp\n\n\n\n\nConvert data to parquet for faster processing\n\n\nCode\ndef spz2parquet(data, force=False):\n    output = f\"../data/{data.name}.parquet\"\n    if Path(output).exists() and not force:\n        logger.info(\"Data already converted to parquet\")\n    else: \n        df = pandas.DataFrame(\n            data.values, index=pandas.Series(data.time, name=\"time\"), columns=data.columns\n        )\n        \n        df.to_parquet(output)\n        logger.info(\"Data converted to parquet successfully\")\n\n\n\n\nCheck and preprocess the data\nAs we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when X, SSE and X, GSE is positive.\n\nState data time resolution is 1 minute…\nFGS data time resolution is 4 second…\n\n\n\nCode\n# get_time_dff(sat_state)\n# get_time_dff(data)\n\n\n\n\nCode\nsat = \"sta\"\nfiles = f\"../data/{sat}_{datatype}_{coord}.parquet\"\noutput = f\"../data/{sat}_data.parquet\"\n\nrename_mapping = {\n    \"Bx FGS-D\": \"BX\",\n    \"By FGS-D\": \"BY\",\n    \"Bz FGS-D\": \"BZ\",\n}\n\nif Path(output).exists():\n    pass\nelse:\n    data = pl.scan_parquet(files).rename(rename_mapping).unique(\"time\").sort(\"time\")\n    data_sw = data.join_asof(sat_state_sw, on=\"time\", tolerance=\"1m\").drop_nulls().collect()\n    data_sw.write_parquet(output)\n\n\n\n\nCode\ndf = (\n    sat_state_sw.upsample(\"time\", every=\"1m\")\n    .group_by_dynamic(\"time\", every=\"1d\")\n    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n    .with_columns(\n        pl.when(pl.col(\"null_count\") &gt; 720).then(0).otherwise(1).alias(\"availablity\")\n    )\n)\n\nproperties = {\n    'width': 800,\n}\n\nchart1 = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='null_count'\n).properties(**properties)\n\nchart2  = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='availablity'\n).properties(**properties)\n\n(chart1 & chart2)\n\n\ndf = ( sat_state_sw.upsample(“time”, every=“1m”) .group_by_dynamic(“time”, every=“1d”) .agg(pl.col(“X, SSE”).null_count().alias(“null_count”)) .with_columns( pl.when(pl.col(“null_count”) &gt; 720).then(0).otherwise(1).alias(“availablity”) ) )\nproperties = { ‘width’: 800, }\nchart1 = alt.Chart(df).mark_point().encode( x=‘time’, y=‘null_count’ ).properties(**properties)\nchart2 = alt.Chart(df).mark_point().encode( x=‘time’, y=‘availablity’ ).properties(**properties)\n(chart1 & chart2)"
  },
  {
    "objectID": "stereo.html#processing-the-whole-data",
    "href": "stereo.html#processing-the-whole-data",
    "title": "IDs from STEREO",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\n\nCode\ndef get_memory_usage(data):\n    datatype = type(data)\n    match datatype:\n        case pl.DataFrame:\n            size = data.estimated_size()\n        case pd.DataFrame:\n            size = data.memory_usage().sum()\n        case xr.DataArray:\n            size = data.nbytes\n\n    logger.info(f\"{naturalsize(size)} ({datatype.__name__})\")\n    return size\n\n\n\n\nCode\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=4)\nfiles = f\"../data/{sat}_data_sw.parquet\"\noutput = f'../data/{sat}_candidates_sw_tau_{tau.seconds}.parquet'\n\ndata = pl.scan_parquet(files).set_sorted('time').collect()\nsat_fgm = df2ts(\n    data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"}\n)\nget_memory_usage(data)\nget_memory_usage(sat_fgm)\n\nindices = compute_indices(data, tau)\n\n# filter condition\nsparse_num = tau / data_resolution // 3\nfilter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n\ncandidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\ncandidates = convert_to_dataframe(candidates_pl)\nget_memory_usage(candidates)\n# del indices\n\n\n2023-09-27 11:57:07.031 | INFO     | __main__:get_memory_usage:11 - 741.8 MB (DataFrame)\n2023-09-27 11:57:07.031 | INFO     | __main__:get_memory_usage:11 - 222.6 MB (DataArray)\n27-Sep-23 11:57:09: UserWarning: Ray execution environment not yet initialized. Initializing...\nTo remove this warning, run the following python code before doing dataframe operations:\n\n    import ray\n    ray.init()\n\n\n27-Sep-23 11:57:11: Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n27-Sep-23 11:57:11: Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n2023-09-27 11:57:12,367 INFO worker.py:1642 -- Started a local Ray instance.\n27-Sep-23 11:57:13: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\nDistributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\nEstimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\nDistributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n2023-09-27 11:57:13.471 | INFO     | __main__:get_memory_usage:11 - 6.3 MB (DataFrame)\n\n\n6335828\n\n\n\n\nCode\nids = process_candidates(candidates, sat_fgm, data, data_resolution)\nids = ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])\nids.write_parquet(output)\n\n\nEstimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n\n\n\n\nCode\ntest_eq(ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).shape, ids.unique(\"d_time\").shape)"
  },
  {
    "objectID": "stereo.html#obsolete-codes",
    "href": "stereo.html#obsolete-codes",
    "title": "IDs from STEREO",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\n\nCode\nimport pycdfpp\nimport pyspedas\n\n\n\n\nCode\ndef convert_thm_state_to_parquet(\n    probe: str, trange\n):\n    file_name = f\"./data/th{probe}_state.parquet\"\n    if os.path.exists(file_name):\n        return file_name\n\n    start = trange.start.to_string()\n    end = trange.end.to_string()\n\n    files = pyspedas.themis.state(\n        probe=probe,\n        trange=[start, end],\n        downloadonly=True,\n        no_update=True,\n    )\n\n    thm_pos_sse_Xs = []\n    thm_pos_gse_Xs = []\n    thm_state_times = []\n    for file in files:\n        thm_state = pycdfpp.load(file)\n        epoch_dt64 = thm_state[\n            f\"time\"\n        ].values  #  CATDESC: \"thm_state_time, UTC, in seconds since 01-Jan-1970 00:00:00\"\n        thm_pos_sse_Xs.append(thm_state[f\"th{probe}_pos_sse\"].values[:, 0])\n        thm_pos_gse_Xs.append(thm_state[f\"th{probe}_pos_gse\"].values[:, 0])\n        thm_state_times.append(epoch_dt64)\n\n    thm_pos_sse_X = np.concatenate(thm_pos_sse_Xs)\n    thm_pos_gse_X = np.concatenate(thm_pos_gse_Xs)\n    thm_state_time = np.concatenate(thm_state_times)\n\n    pl.DataFrame(\n        {\n            \"thm_state_time\": thm_state_time,\n            \"thm_pos_gse_X\": thm_pos_gse_X,\n            \"thm_pos_sse_X\": thm_pos_sse_X,\n        }\n    ).with_columns(\n        pl.from_epoch(pl.col(\"thm_state_time\"), time_unit=\"s\")\n    ).write_parquet(\n        file_name\n    )\n\n    return file_name\n\n\ndef convert_thm_fgm_to_parquet(probe, trange):\n    file_name = f\"./data/th{probe}_fgm.parquet\"\n    if os.path.exists(file_name):\n        return file_name\n\n    start = trange.start.to_string()\n    end = trange.end.to_string()\n    \n    files = pyspedas.themis.fgm(\n        probe=probe,\n        trange=[start, end],\n        downloadonly=True,\n        no_update=True,\n    )\n\n    thm_fgl_gses = []\n    thm_fgl_btotals = []\n    thm_fgl_times = []\n\n    for file in files:\n        cdf = pycdfpp.load(file)\n        thm_fgl_gses.append(cdf[f\"th{probe}_fgl_gse\"].values)\n        thm_fgl_btotals.append(cdf[f\"th{probe}_fgl_btotal\"].values)\n        thm_fgl_times.append(cdf[f\"th{probe}_fgl_time\"].values)\n\n    thm_fgl_gse = np.concatenate(thm_fgl_gses)\n    thm_fgl_btotal = np.concatenate(thm_fgl_btotals)\n    thm_fgl_time = np.concatenate(thm_fgl_times)\n\n    pl.DataFrame(\n        {\n            \"time\": thm_fgl_time,\n            \"BX\": thm_fgl_gse[:,0],\n            \"BY\": thm_fgl_gse[:,1],\n            \"BZ\": thm_fgl_gse[:,2],\n            \"B\": thm_fgl_btotal,\n        }\n    ).with_columns(\n        pl.from_epoch(pl.col(\"thm_fgl_time\"), time_unit=\"s\"),\n    ).write_parquet(   \n        file_name\n    )\n    \n    return file_name\n\n\n\n\nCode\nconvert_thm_state_to_parquet(probe, trange)\nconvert_thm_fgm_to_parquet(probe, trange)"
  },
  {
    "objectID": "juno.html",
    "href": "juno.html",
    "title": "IDs from Juno",
    "section": "",
    "text": "import all the packages needed for the project\nfrom ids_finder.utils import *\nfrom ids_finder.core import *\nfrom fastcore.utils import *\nfrom fastcore.test import *\n\nimport polars as pl\ntry:\n    import modin.pandas as pd\n    import modin.pandas as mpd\nexcept ImportError:\n    import pandas as pd\n\nimport pandas\n\nfrom datetime import timedelta\n\nfrom loguru import logger\n\nimport pdr\n\n\nimport pytplot\nfrom pytplot import timebar\nfrom pytplot import get_data, store_data, tplot, split_vec, join_vec, tplot_options, options, tlimit, highlight, degap\n\nimport pdpipe as pdp\n\nfrom typing import Callable\nfrom pandas import (\n    DataFrame,\n)"
  },
  {
    "objectID": "juno.html#background",
    "href": "juno.html#background",
    "title": "IDs from Juno",
    "section": "Background",
    "text": "Background\nSpacecraft-Solar equatorial\n\nCoordinate System of Data\n\nSE (Solar Equatorial)\n\nCode: se\nResampling options:\n\nNumber of seconds (1 or 60): se_rN[N]s\nResampled 1 hour: se_r1h\n\n\nPC (Planetocentric)\n\nCode: pc\nResampling options:\n\nNumber of seconds (1 or 60): pc_rN[N]s\n\n\nSS (Sun-State)\n\nCode: ss\nResampling options:\n\nNumber of seconds (1 or 60): ss_rN[N]s\n\n\nPL (Payload)\n\nCode: pl\nResampling options:\n\nNumber of seconds (1 or 60): pl_rN[N]s\n\n\n\n------------------------------------------------------------------------------\nJuno Mission Phases                                                           \n------------------------------------------------------------------------------\nStart       Mission                                                           \nDate        Phase                                                             \n==============================================================================\n2011-08-05  Launch                                                            \n2011-08-08  Inner Cruise 1                                                    \n2011-10-10  Inner Cruise 2                                                    \n2013-05-28  Inner Cruise 3                                                    \n2013-11-05  Quiet Cruise                                                      \n2016-01-05  Jupiter Approach                                                  \n2016-06-30  Jupiter Orbital Insertion                                         \n2016-07-05  Capture Orbit                                                     \n2016-10-19  Period Reduction Maneuver                                         \n2016-10-20  Orbits 1-2                                                        \n2016-11-09  Science Orbits                                                    \n2017-10-11  Deorbit\nFile Naming Convention                                                        \n==============================================================================\nConvention:                                                                   \n   fgm_jno_LL_CCYYDDDxx_vVV.ext                                               \nWhere:                                                                        \n   fgm - Fluxgate Magnetometer three character instrument abbreviation        \n   jno - Juno                                                                 \n    LL - CODMAC Data level, for example, l3 for level 3                       \n    CC - The century portion of a date, 20                                    \n    YY - The year of century portion of a date, 00-99                         \n   DDD - The day of year, 001-366                                             \n    xx - Coordinate system of data (se = Solar equatorial, ser = Solar        \n         equatorial resampled, pc = Planetocentric, ss = Sun-State,           \n         pl = Payload)                                                        \n     v - separator to denote Version number                                   \n    VV - version                                                              \n   ext - file extension (sts = Standard Time Series (ASCII) file, lbl = Label \n         file)                                                                \nExample:                                                                      \n   fgm_jno_l3_2014055se_v00.sts"
  },
  {
    "objectID": "juno.html#dataset-overview",
    "href": "juno.html#dataset-overview",
    "title": "IDs from Juno",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nCode\npds_dir = \"https://pds-ppi.igpp.ucla.edu/data\"\n\npossible_coords = [\"se\", \"ser\", \"pc\", \"ss\", \"pl\"]\npossible_exts = [\"sts\", \"lbl\"]\npossible_data_rates = [\"1s\", \"1min\", \"1h\"]\n\njuno_ss_config = {\n    \"DATA_SET_ID\": \"JNO-SS-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\njuno_j_config = {\n    \"DATA_SET_ID\": \"JNO-J-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\n\n\nCheck the data\n\n\nDownload all the files\n\n\nCode\n# wget -r --no-parent --no-clobber 'https://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC/'\n# aria2c -x 16 -s 16 'https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC'\n\n\n1 day of data resampled by 1 sec is about 12 MB.\nSo 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\nDownloading rate is about 250 KB/s, so it will take about 3 days to download all the data.\n\n\nCode\nnum_of_files = 6*365\njno_file_size = 12e3\nthm_file_size = 40e3\nfiles_size = jno_file_size + thm_file_size\ndownloading_rate = 250\nprocessing_rate = 1/60\n\ntime_to_download = num_of_files * files_size / downloading_rate / 60 / 60\nspace_required = num_of_files * files_size / 1e6\ntime_to_process = num_of_files / processing_rate / 60 / 60\n\nprint(f\"Time to download: {time_to_download:.2f} hours\")\nprint(f\"Disk space required: {space_required:.2f} GB\")\nprint(f\"Time to process: {time_to_process:.2f} hours\")\n\n\n\n\nConvert all files from lbl to parquet for faster processing\n\n\nCode\nimport pdr\n\n\n\n\nConvert data from lbl to parquet format\ndef lbl2parquet(src: Path, dest: Path) -&gt; None:\n    df = pdr.read(src).TABLE\n    df.to_parquet(dest)\n\n\ndef convert_file(\n    file_path: Path, target_format: str, conversion_func: Callable, check_exist=True\n) -&gt; None:\n    target_suffix = (\n        target_format if target_format.startswith(\".\") else f\".{target_format}\"\n    )\n    target_file = file_path.with_suffix(target_suffix)\n\n    if check_exist and target_file.exists():\n        return True\n\n    try:\n        conversion_func(file_path, target_file)\n    except Exception as e:\n        logger.error(f\"Error converting {file_path} to {target_file}: {e}\")\n        return False\n\n@startthread\ndef convert_files():\n    format_from = \"lbl\"\n    format_to = \"parquet\"\n    local_dir = Path.home() / \"data/juno\"\n    pattern = f\"**/*.{format_from}\"\n    convert_func = lbl2parquet\n    for file in local_dir.glob(pattern):\n        convert_file(file, format_to, convert_func)\n    logger.info(\"Done converting files\")\n\n\n\n\nCode\n# delete all files with extension\n# find . -type f -name '*.parquet' -delete\n# find . -type f -name '*.orc' -delete\n# find . -type f -name '*.lbl' -delete\n\n\n\n\nCode\ndef _batch_pre_process(year, force=False):\n    trange = [f\"{year}-01-01\", f\"{year+1}-01-01T01\"]  # having some overlap\n    dir_path = Path.home() /  \"data/juno/JNO-SS-3-FGM-CAL-V1.0/\"\n    pattern = \"**/*.parquet\"\n    data = dir_path / pattern\n    \n    output = Path(f\"../data/jno_{year}.parquet\")\n    output.parent.mkdir(parents=True, exist_ok=True)\n    if os.path.exists(output) and not force:\n        logger.info(f\"File {output} exists. Skipping\")\n        return output\n    logger.info(f\"Preprocessing data for year {year}\")\n    \n    lazy_df = pl.scan_parquet(data)\n    temp_df = (\n        lazy_df.filter(\n            pl.col(\"time\").is_between(pd.Timestamp(trange[0]), pd.Timestamp(trange[1])),\n        )\n        .sort(\n            \"time\"\n        )  # needed for `compute_index_std` to work properly as `group_by_dynamic` requires the data to be sorted\n        .filter(\n            pl.col(\n                \"time\"\n            ).is_first_distinct()  # remove duplicate time values for xarray to select data properly, though significantly slows down the computation\n        )\n        .rename({\"BX SE\": \"BX\", \"BY SE\": \"BY\", \"BZ SE\": \"BZ\"})\n    )\n    temp_df.collect().write_parquet(output)\n    return output\n\n@startthread\ndef batch_pre_process():\n    starting_year = starting_date.year\n    ending_year = ending_date.year\n\n    for year in range(starting_year, ending_year+1):\n        _batch_pre_process(year)"
  },
  {
    "objectID": "juno.html#processing-the-whole-data",
    "href": "juno.html#processing-the-whole-data",
    "title": "IDs from Juno",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\n\nCode\nfrom tqdm import tqdm\n\n\n\n\nCode\nsat = 'jno'\ncoord = 'se'\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=1)\n\ndef batch_process():\n    for year in tqdm(range(starting_date.year, ending_date.year+1)):\n        files = f'../data/{sat}_{year}.parquet'\n        output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n        \n        if os.path.exists(output):\n            logger.info(f\"Skipping {year} as the output file already exists.\")\n            continue\n\n        data = pl.scan_parquet(files).set_sorted('time').collect()\n        sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n\n        indices = compute_indices(data, tau)\n        # filter condition\n        sparse_num = tau / data_resolution // 3\n        filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n\n        candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n        candidates = convert_to_dataframe(candidates_pl)\n        \n        ids = process_candidates(candidates, sat_fgm, data, data_resolution)\n        ids.write_parquet(output)\n\n\n\n\nCode\nbatch_process()\n\n\n\n\nCombine all the files into one and remove duplicates\nfiles = f'../data/{sat}_candidates_*_tau_{tau.seconds}.parquet'\noutput = f'../data/{sat}_candidates_tau_{tau.seconds}.parquet'\nids = pl.scan_parquet(files).unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).collect()\nids.write_parquet(output)"
  },
  {
    "objectID": "juno.html#obsolete-codes",
    "href": "juno.html#obsolete-codes",
    "title": "IDs from Juno",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\nDownload and read file from the server (one by one)\n\n\nCode\ndef juno_load_fgm(trange: list, coord=\"se\", data_rate=\"1s\") -&gt; DataFrame:\n    \"\"\"\n    Get the data array for a given time range and coordinate.\n\n    Parameters:\n        trange (list): The time range.\n        coord (str, optional): The coordinate. Defaults to 'se'.\n        data_rate (str, optional): The data rate. Defaults to '1s'.\n\n    Returns:\n        pandas.DataFrame: The dataframe for the given time range and coordinate.\n    \"\"\"\n\n    if len(trange) != 2:\n        raise ValueError(\n            \"Expected trange to have exactly 2 elements: start and stop time.\"\n        )\n\n    start_time = pandas.Timestamp(trange[0])\n    stop_time = pandas.Timestamp(trange[1])\n\n    temp_index_df = index_df[\n        (index_df[\"SID\"] == get_sid(coord, data_rate))\n    ].reset_index(drop=True)\n\n    # Filtering\n    relevant_files = temp_index_df[\n        (temp_index_df[\"STOP_TIME\"] &gt; start_time)\n        & (temp_index_df[\"START_TIME\"] &lt; stop_time)\n    ]\n    dataframes = [download_and_read_lbl_file(row) for _, row in relevant_files.iterrows()]\n\n    # rows = [row for _, row in relevant_files.iterrows()]\n    # with concurrent.futures.ThreadPoolExecutor() as executor:\n    #     dataframes = list(executor.map(download_and_read_file, rows))\n\n    combined_data = pandas.concat(dataframes)\n\n    return pdp_process_juno_df(combined_data)\n\ndef get_sid(coord, data_rate):\n    sid_mapping = {\n        \"pc\": {\"1s\": \"PC 1 SECOND\", \"1min\": \"PC 1 MINUTE\", \"\": \"PCENTRIC\"},\n        \"pl\": {\"1s\": \"PAYLOAD 1 SECOND\", \"\": \"PAYLOAD\"},\n        \"ss\": {\"1s\": \"SS 1 SECOND\", \"1min\": \"SS 1 MINUTE\", \"\": \"SUNSTATE\"},\n        \"se\": {\"1s\": \"SE 1 SECOND\", \"1min\": \"SE 1 MINUTE\", \"\": \"SE\"},\n    }\n    try:\n        return sid_mapping[coord][data_rate]\n    except KeyError:\n        return None\n\n_skip_cond = ~pdp.cond.HasAllColumns([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"])\npdp_process_juno_df = pdp.PdPipeline(\n    [\n        pdp.ColByFrameFunc(\n            \"time\",\n            lambda df: pandas.to_datetime(df[\"SAMPLE UTC\"], format=\"%Y %j %H %M %S %f\"),\n            skip=_skip_cond,\n        ),\n        pdp.ColDrop([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"], skip=_skip_cond),\n        pdp.df.set_index(\"time\"),\n        pdp.ColRename(col_renamer)\n        # pdp.AggByCols('SAMPLE UTC', func=lambda time: pandas.to_datetime(time, format='%Y %j %H %M %S %f'), func_desc='Convert time to datetime') # NOTE: this is quite slow\n        # pdp.df['time'] &lt;&lt; pandas.to_datetime(pdp.df['SAMPLE UTC'], format='%Y %j %H %M %S %f'), # NOTE: this is not work\n    ],\n)"
  },
  {
    "objectID": "ids_finder.html",
    "href": "ids_finder.html",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "The first index is \\[ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} \\] The second index is \\[ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} \\] The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\nthird index (relative field jump) is \\[ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\] a supplementary condition toreduce the uncertainty of recognition\n\n\n\n\nCode\nhelp(compute_index_std)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_candidates (candidates:modin.pandas.dataframe.DataFrame,\n                 candidate_type=None, num:int=4)\n\n\n\n\n\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data\n\n\n\n\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data"
  },
  {
    "objectID": "ids_finder.html#id-identification",
    "href": "ids_finder.html#id-identification",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "The first index is \\[ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} \\] The second index is \\[ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} \\] The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\nthird index (relative field jump) is \\[ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\] a supplementary condition toreduce the uncertainty of recognition\n\n\n\n\nCode\nhelp(compute_index_std)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_candidates (candidates:modin.pandas.dataframe.DataFrame,\n                 candidate_type=None, num:int=4)\n\n\n\n\n\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data\n\n\n\n\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data"
  },
  {
    "objectID": "ids_finder.html#plotting",
    "href": "ids_finder.html#plotting",
    "title": "Finding magnetic discontinuities",
    "section": "Plotting",
    "text": "Plotting\n\nsource\n\nplot_candidates\n\n plot_candidates (candidates:pandas.core.frame.DataFrame,\n                  candidate_type=None, num=4, plot_func=&lt;function\n                  plot_candidate&gt;)\n\nPlot a set of candidates.\nParameters: - candidates (pd.DataFrame): DataFrame containing the candidates. - candidate_type (str, optional): Filter candidates based on a specific type. - num (int): Number of candidates to plot, selected randomly. - plot_func (callable): Function used to plot an individual candidate.\n\nsource\n\n\nplot_candidate\n\n plot_candidate (candidate:pandas.core.series.Series,\n                 sat_fgm:xarray.core.dataarray.DataArray,\n                 tau:datetime.timedelta)\n\n\nsource\n\n\nformat_candidate_title\n\n format_candidate_title (candidate:pandas.core.series.Series)\n\n\nsource\n\n\nplot_basic\n\n plot_basic (data:xarray.core.dataarray.DataArray,\n             tstart:pandas._libs.tslibs.timestamps.Timestamp,\n             tstop:pandas._libs.tslibs.timestamps.Timestamp,\n             tau:datetime.timedelta, mva_tstart=None, mva_tstop=None,\n             neighbor:int=1)\n\n\nsource\n\n\ntime_stamp\n\n time_stamp (ts)\n\nReturn POSIX timestamp as float."
  },
  {
    "objectID": "ids_finder.html#id-parameters",
    "href": "ids_finder.html#id-parameters",
    "title": "Finding magnetic discontinuities",
    "section": "ID parameters",
    "text": "ID parameters\n\nDuration\nDefinitions of duration - Define $d^* = ( | dB / dt | ) $, and then define time interval where \\(| dB/dt |\\) decreases to \\(d^*/4\\)\n\nsource\n\n\nget_time_from_condition\n\n get_time_from_condition (vec:xarray.core.dataarray.DataArray, threshold,\n                          condition_type)\n\n\nsource\n\n\nfind_start_end_times\n\n find_start_end_times (vec_diff_mag:xarray.core.dataarray.DataArray,\n                       d_time, threshold)\n\n\nsource\n\n\ncalc_d_duration\n\n calc_d_duration (vec:xarray.core.dataarray.DataArray, d_time, threshold)\n\n\nsource\n\n\ncalc_duration\n\n calc_duration (vec:xarray.core.dataarray.DataArray, threshold_ratio=0.25)\n\n\nsource\n\n\ncalc_candidate_d_duration\n\n calc_candidate_d_duration (candidate, data)\n\n\nsource\n\n\ncalc_candidate_duration\n\n calc_candidate_duration (candidate:modin.pandas.series.Series, data)\n\n\nsource\n\n\ncalibrate_candidate_duration\n\n calibrate_candidate_duration (candidate:modin.pandas.series.Series,\n                               data:xarray.core.dataarray.DataArray,\n                               data_resolution, ratio=0.75)\n\nCalibrates the candidate duration. - If only one of ‘d_tstart’ or ‘d_tstop’ is provided, calculates the missing one based on the provided one and ‘d_time’. - Then if this is not enough points between ‘d_tstart’ and ‘d_tstop’, returns None for both.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncandidate\nSeries\n\n\n\n\ndata\nDataArray\n\n\n\n\ndata_resolution\n\n\n\n\n\nratio\nfloat\n0.75\n\n\n\nReturns\n- pd.Series: The calibrated candidate.\n\n\n\n\n\n\n\nID classification\nIn this method, TDs and RDs satisfy $ &lt; 0.2$ and $ | | &gt; 0.4$ B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with &lt; 0.4 B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as &gt; 0.4 B BN bg ∣∣ ∣∣ , &gt; D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\nCriteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\n\n\n\nType\n\\(\\|B_n/B\\|\\)\n\\(\\| \\Delta B / B \\|\\)\n\n\n\n\nRotational (RD)\nlarge\nsmall\n\n\nTangential (TD)\nsmall\nlarge\n\n\nEither (ED)\nsmall\nsmall\n\n\nNeither (ND)\nlarge\nlarge\n\n\n\n\nminimum variance analysis (MVA)\nTo ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis.\n\nsource\n\n\n\ncalc_classification_index\n\n calc_classification_index (data:xarray.core.dataarray.DataArray)\n\n\nsource\n\n\nclassify_id\n\n classify_id (BnOverB, dBOverB)\n\n\n\nField rotation angles\nThe PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)…\n\nsource\n\n\ncalc_candidate_rotation_angle\n\n calc_candidate_rotation_angle (candidates,\n                                data:xarray.core.dataarray.DataArray)\n\nComputes the rotation angle(s) at two different time steps.\n\nsource\n\n\ncalc_rotation_angle\n\n calc_rotation_angle (v1, v2)\n\nComputes the rotation angle between two vectors.\nParameters: - v1: The first vector. - v2: The second vector.\n\n\nAssign satellite locations to the discontinuities\n\nsource\n\n\nget_candidate_location\n\n get_candidate_location (candidate,\n                         location_data:xarray.core.dataarray.DataArray)"
  },
  {
    "objectID": "ids_finder.html#processing-the-whole-dataset",
    "href": "ids_finder.html#processing-the-whole-dataset",
    "title": "Finding magnetic discontinuities",
    "section": "Processing the whole dataset",
    "text": "Processing the whole dataset\n\nsource\n\nget_ID_filter_condition\n\n get_ID_filter_condition (index_std_threshold=2, index_fluc_threshold=1,\n                          index_diff_threshold=0.1, sparse_num=15)\n\n\nsource\n\n\ncalc_candidate_classification_index\n\n calc_candidate_classification_index (candidate, data)\n\n\nsource\n\n\nconvert_to_dataframe\n\n convert_to_dataframe (data:polars.dataframe.frame.DataFrame)\n\nconvert data into a pandas/modin DataFrame\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nDataFrame\norignal Dataframe\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\nIDsPipeline\n\n IDsPipeline ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nprocess_candidates\n\n process_candidates (candidates:polars.dataframe.frame.DataFrame,\n                     sat_fgm:xarray.core.dataarray.DataArray,\n                     sat_state:polars.dataframe.frame.DataFrame,\n                     data_resolution:datetime.timedelta)\n\n\n\n\n\nType\nDetails\n\n\n\n\ncandidates\nDataFrame\npotential candidates DataFrame\n\n\nsat_fgm\nDataArray\nsatellite FGM data\n\n\nsat_state\nDataFrame\nsatellite state data\n\n\ndata_resolution\ntimedelta\ntime resolution of the data\n\n\nReturns\nDataFrame\nprocessed candidates DataFrame"
  },
  {
    "objectID": "ids_finder.html#candidate-class",
    "href": "ids_finder.html#candidate-class",
    "title": "Finding magnetic discontinuities",
    "section": "Candidate class",
    "text": "Candidate class\n\nsource\n\nCandidateID\n\n CandidateID (time, df:polars.dataframe.frame.DataFrame)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "ids_finder.html#test-case",
    "href": "ids_finder.html#test-case",
    "title": "Finding magnetic discontinuities",
    "section": "Test case",
    "text": "Test case\n\n\nCode\nsat = 'jno'\ncoord = 'se'\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=1)\n\nif True:\n    year = 2011\n    files = f'../data/{sat}_{year}.parquet'\n    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n\n    data = pl.scan_parquet(files).set_sorted('time').collect()\n    sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n    sat_state = df2ts(data, [\"X\", \"Y\", \"Z\"], attrs={\"coordinate_system\": coord, \"units\": \"km\"})\n\n    indices = compute_indices(data, tau)\n    # filter condition\n    sparse_num = tau / data_resolution // 3\n    filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n\n    candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n    candidates_pd = candidates_pl.to_pandas()\n    candidates_modin = pd.DataFrame(candidates_pd)\n\n\n\n\nTest different libraries to parallelize the computation\nif True:\n    pdp_test = pdp.ApplyToRows(\n        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, jno_fgm)),\n        # lambda candidate: calc_duration(jno_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n        func_desc=\"calculating duration parameters\",\n    )\n    \n    # process_candidates(candidates_modin, sat_fgm, sat_state, data_resolution)\n    \n    # ---\n    # successful cases\n    # ---\n    # candidates_pd.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # Standard case: 37+s secs\n    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 80 secs\n    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 60 secs\n    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works, 8 secs # not work? `DataFrame' object has no attribute 'mapply'\n    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works with ray, 8 secs # NOTE: can not work with dask\n    # pdp_test(candidates_modin) # this works, 8 secs\n    \n    # ---\n    # failed cases\n    # ---\n    # candidates_modin.apply(calc_candidate_duration, axis=1, data=jno_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n    # pdp_test(candidates_modin) # TypeError: Unexpected type generated by applying a function to a DataFrame. Only Series and DataFrame are allowed."
  },
  {
    "objectID": "ids_finder.html#obsolete-codes",
    "href": "ids_finder.html#obsolete-codes",
    "title": "Finding magnetic discontinuities",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\nprocess_candidates\nAssign coordinates using Dataframe.apply is not optimized, quite slow…\n\n\nCode\ndef process_candidates(\n    candidates: pd.DataFrame, # potential candidates DataFrame\n    sat_fgm: xr.DataArray, # satellite FGM data\n    sat_state: xr.DataArray, # satellite state data\n    data_resolution: timedelta, # time resolution of the data\n) -&gt; pd.DataFrame: # processed candidates DataFrame\n    id_pipelines = IDsPipeline()\n\n    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n\n    # calibrate duration\n    temp_candidates = candidates.loc[\n        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n\n    if not temp_candidates.empty:\n        candidates.update(\n            id_pipelines.calibrate_duration(sat_fgm, data_resolution).apply(\n                temp_candidates\n            )\n        )\n\n    ids = (\n        id_pipelines.classify_id(sat_fgm)\n        + id_pipelines.calc_rotation_angle(sat_fgm)\n        + id_pipelines.assign_coordinates(sat_state)\n    ).apply(\n        candidates.dropna()  # Remove candidates with NaN values)\n    )\n\n    return ids"
  },
  {
    "objectID": "themis.html",
    "href": "themis.html",
    "title": "IDs from ARTHEMIS",
    "section": "",
    "text": "import all the packages needed for the project\nfrom ids_finder.utils import *\nfrom ids_finder.core import *\nfrom fastcore.utils import *\nfrom fastcore.test import *\n\nimport polars as pl\ntry:\n    import modin.pandas as pd\n    import modin.pandas as mpd\nexcept ImportError:\n    import pandas as pd\n\nimport pandas\nimport numpy as np\nfrom humanize import naturalsize\nimport xarray as xr\n\n\nfrom datetime import timedelta\nfrom loguru import logger\nimport speasy as spz\nfrom multipledispatch import dispatch\n\nimport altair as alt"
  },
  {
    "objectID": "themis.html#dataset-overview",
    "href": "themis.html#dataset-overview",
    "title": "IDs from ARTHEMIS",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nCode\nartemis_probes = [\"b\", \"c\"]\nprobe = artemis_probes[0]\n\njno_start_date = \"2011-08-25\"\njno_end_date = \"2016-06-30\" \n\ntrange = [jno_start_date, jno_end_date]\ntest_trange = [\"2011-08-25\", \"2011-09-25\"]\n\n\n\nDownload all the files\n\n\nCode\nsat = 'thb'\ncoord = 'gse'\ndatatype  = 'fgs'\n\nsat_fgm_product = f'cda/{sat.upper()}_L2_FGM/{sat}_fgs_gse'\nsat_pos_sse_product = f'cda/{sat.upper()}_L1_STATE/{sat}_pos_sse'\nsat_pos_gse_product = f'cda/{sat.upper()}_L1_STATE/{sat}_pos_gse'\n\nproducts = [\n    sat_fgm_product,\n    sat_pos_sse_product,\n    sat_pos_gse_product\n]\n\n\n\n\nCode\n@threaded\ndef download_data(products, trange):\n    logger.info(\"Downloading data\")\n    spz.get_data(products, trange, progress=True, disable_proxy=True)\n    logger.info(\"Data downloaded\")\n    # spz.get_data(products, jno_start_date, jno_end_date)\n\n\nDownload data in a background thread\n\n\nCode\ndownload_data(products, trange)\n\n\n#| eval: false download_data(products, trange)\n\n\n\n\nConvert data to parquet for faster processing\n\n\nCode\ndef spz2parquet(data, force=False):\n    output = f\"../data/{data.name}.parquet\"\n    if Path(output).exists() and not force:\n        logger.info(\"Data already converted to parquet\")\n    else: \n        df = pandas.DataFrame(\n            data.values, index=pandas.Series(data.time, name=\"time\"), columns=data.columns\n        )\n        \n        df.to_parquet(output)\n        logger.info(\"Data converted to parquet successfully\")\n\n\n\n\nCode\ndataset = spz.get_data(products, trange)\n\nfor data in dataset:\n    spz2parquet(data, force=False)\n\n\ndataset = spz.get_data(products, trange)\nfor data in dataset: spz2parquet(data, force=False)\n\n\n\n\nCode\ndef thm_rename_col(col: str):\n    if \",\" in col:\n        col = col.split(\",\")[0]\n    return col.split()[0].upper()\n\n\n\n\nCheck and preprocess the data\nAs we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when X, SSE and X, GSE is positive.\n\nState data time resolution is 1 minute…\nFGS data time resolution is 4 second…\n\n\n\nCode\ndef get_thm_state(sat):\n    sat_pos_sse_files = f\"../data/{sat}_pos_sse.parquet\"\n    sat_pos_sse = pl.scan_parquet(sat_pos_sse_files).set_sorted(\"time\")\n    sat_pos_gse_files = f\"../data/{sat}_pos_gse.parquet\"\n    sat_pos_gse = pl.scan_parquet(sat_pos_gse_files).set_sorted(\"time\")\n    sat_state = sat_pos_sse.join(sat_pos_gse, on=\"time\", how=\"inner\")\n    return sat_state\n\n\n\n\nCode\n@dispatch(pl.DataFrame)\ndef calc_time_diff(data: pl.DataFrame): \n    return data.get_column('time').diff(null_behavior=\"drop\").unique().sort()\n\n@dispatch(pl.LazyFrame)\ndef calc_time_diff(\n    data: pl.LazyFrame\n) -&gt; pl.Series: \n    return calc_time_diff(data.collect())\n\n# get_time_dff(sat_state)\n# get_time_dff(data)\n\n\n\n\nCode\nsat = \"thb\"\ncoord = \"gse\"\ndatatype = \"fgs\"\nfiles = f\"../data/{sat}_{datatype}_{coord}.parquet\"\nrename_mapping = {\n    \"Bx FGS-D\": \"BX\",\n    \"By FGS-D\": \"BY\",\n    \"Bz FGS-D\": \"BZ\",\n}\n\n\noutput = f\"../data/{sat}_data_sw.parquet\"\nif Path(output).exists():\n    pass\nelse:\n    sat_state = get_thm_state(sat).collect()\n    sat_state_sw = sat_state.filter((pl.col(\"X, SSE\") &gt;= 0) & (pl.col(\"X, GSE\") &gt;= 0))\n    data = pl.scan_parquet(files).rename(rename_mapping).unique(\"time\").sort(\"time\")\n    data_sw = data.join_asof(sat_state_sw, on=\"time\", tolerance=\"1m\").drop_nulls().collect()\n    data_sw.write_parquet(output)\n\n\n\n\nCode\ndf = (\n    sat_state_sw.upsample(\"time\", every=\"1m\")\n    .group_by_dynamic(\"time\", every=\"1d\")\n    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n    .with_columns(\n        pl.when(pl.col(\"null_count\") &gt; 720).then(0).otherwise(1).alias(\"availablity\")\n    )\n)\n\nproperties = {\n    'width': 800,\n}\n\nchart1 = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='null_count'\n).properties(**properties)\n\nchart2  = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='availablity'\n).properties(**properties)\n\n(chart1 & chart2)\n\n\ndf = ( sat_state_sw.upsample(“time”, every=“1m”) .group_by_dynamic(“time”, every=“1d”) .agg(pl.col(“X, SSE”).null_count().alias(“null_count”)) .with_columns( pl.when(pl.col(“null_count”) &gt; 720).then(0).otherwise(1).alias(“availablity”) ) )\nproperties = { ‘width’: 800, }\nchart1 = alt.Chart(df).mark_point().encode( x=‘time’, y=‘null_count’ ).properties(**properties)\nchart2 = alt.Chart(df).mark_point().encode( x=‘time’, y=‘availablity’ ).properties(**properties)\n(chart1 & chart2)"
  },
  {
    "objectID": "themis.html#processing-the-whole-data",
    "href": "themis.html#processing-the-whole-data",
    "title": "IDs from ARTHEMIS",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\n\nCode\ndef get_memory_usage(data):\n    datatype = type(data)\n    match datatype:\n        case pl.DataFrame:\n            size = data.estimated_size()\n        case pd.DataFrame:\n            size = data.memory_usage().sum()\n        case xr.DataArray:\n            size = data.nbytes\n\n    logger.info(f\"{naturalsize(size)} ({datatype.__name__})\")\n    return size\n\n\n\n\nCode\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=4)\nfiles = f\"../data/{sat}_data_sw.parquet\"\noutput = f'../data/{sat}_candidates_sw_tau_{tau.seconds}.parquet'\n\ndata = pl.scan_parquet(files).set_sorted('time').collect()\nsat_fgm = df2ts(\n    data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"}\n)\nget_memory_usage(data)\nget_memory_usage(sat_fgm)\n\nindices = compute_indices(data, tau)\n\n# filter condition\nsparse_num = tau / data_resolution // 3\nfilter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n\ncandidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\ncandidates = convert_to_dataframe(candidates_pl)\nget_memory_usage(candidates)\n# del indices\n\n\n2023-09-27 11:57:07.031 | INFO     | __main__:get_memory_usage:11 - 741.8 MB (DataFrame)\n2023-09-27 11:57:07.031 | INFO     | __main__:get_memory_usage:11 - 222.6 MB (DataArray)\n27-Sep-23 11:57:09: UserWarning: Ray execution environment not yet initialized. Initializing...\nTo remove this warning, run the following python code before doing dataframe operations:\n\n    import ray\n    ray.init()\n\n\n27-Sep-23 11:57:11: Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n27-Sep-23 11:57:11: Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n2023-09-27 11:57:12,367 INFO worker.py:1642 -- Started a local Ray instance.\n27-Sep-23 11:57:13: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\nDistributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\nEstimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\nDistributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n2023-09-27 11:57:13.471 | INFO     | __main__:get_memory_usage:11 - 6.3 MB (DataFrame)\n\n\n6335828\n\n\n\n\nCode\nids = process_candidates(candidates, sat_fgm, data, data_resolution)\nids = ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])\nids.write_parquet(output)\n\n\nEstimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n\n\n\n\nCode\ntest_eq(ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).shape, ids.unique(\"d_time\").shape)"
  },
  {
    "objectID": "themis.html#obsolete-codes",
    "href": "themis.html#obsolete-codes",
    "title": "IDs from ARTHEMIS",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\n\nCode\nimport pycdfpp\nimport pyspedas\n\n\n\n\nCode\ndef convert_thm_state_to_parquet(\n    probe: str, trange\n):\n    file_name = f\"./data/th{probe}_state.parquet\"\n    if os.path.exists(file_name):\n        return file_name\n\n    start = trange.start.to_string()\n    end = trange.end.to_string()\n\n    files = pyspedas.themis.state(\n        probe=probe,\n        trange=[start, end],\n        downloadonly=True,\n        no_update=True,\n    )\n\n    thm_pos_sse_Xs = []\n    thm_pos_gse_Xs = []\n    thm_state_times = []\n    for file in files:\n        thm_state = pycdfpp.load(file)\n        epoch_dt64 = thm_state[\n            f\"time\"\n        ].values  #  CATDESC: \"thm_state_time, UTC, in seconds since 01-Jan-1970 00:00:00\"\n        thm_pos_sse_Xs.append(thm_state[f\"th{probe}_pos_sse\"].values[:, 0])\n        thm_pos_gse_Xs.append(thm_state[f\"th{probe}_pos_gse\"].values[:, 0])\n        thm_state_times.append(epoch_dt64)\n\n    thm_pos_sse_X = np.concatenate(thm_pos_sse_Xs)\n    thm_pos_gse_X = np.concatenate(thm_pos_gse_Xs)\n    thm_state_time = np.concatenate(thm_state_times)\n\n    pl.DataFrame(\n        {\n            \"thm_state_time\": thm_state_time,\n            \"thm_pos_gse_X\": thm_pos_gse_X,\n            \"thm_pos_sse_X\": thm_pos_sse_X,\n        }\n    ).with_columns(\n        pl.from_epoch(pl.col(\"thm_state_time\"), time_unit=\"s\")\n    ).write_parquet(\n        file_name\n    )\n\n    return file_name\n\n\ndef convert_thm_fgm_to_parquet(probe, trange):\n    file_name = f\"./data/th{probe}_fgm.parquet\"\n    if os.path.exists(file_name):\n        return file_name\n\n    start = trange.start.to_string()\n    end = trange.end.to_string()\n    \n    files = pyspedas.themis.fgm(\n        probe=probe,\n        trange=[start, end],\n        downloadonly=True,\n        no_update=True,\n    )\n\n    thm_fgl_gses = []\n    thm_fgl_btotals = []\n    thm_fgl_times = []\n\n    for file in files:\n        cdf = pycdfpp.load(file)\n        thm_fgl_gses.append(cdf[f\"th{probe}_fgl_gse\"].values)\n        thm_fgl_btotals.append(cdf[f\"th{probe}_fgl_btotal\"].values)\n        thm_fgl_times.append(cdf[f\"th{probe}_fgl_time\"].values)\n\n    thm_fgl_gse = np.concatenate(thm_fgl_gses)\n    thm_fgl_btotal = np.concatenate(thm_fgl_btotals)\n    thm_fgl_time = np.concatenate(thm_fgl_times)\n\n    pl.DataFrame(\n        {\n            \"time\": thm_fgl_time,\n            \"BX\": thm_fgl_gse[:,0],\n            \"BY\": thm_fgl_gse[:,1],\n            \"BZ\": thm_fgl_gse[:,2],\n            \"B\": thm_fgl_btotal,\n        }\n    ).with_columns(\n        pl.from_epoch(pl.col(\"thm_fgl_time\"), time_unit=\"s\"),\n    ).write_parquet(   \n        file_name\n    )\n    \n    return file_name\n\n\n\n\nCode\nconvert_thm_state_to_parquet(probe, trange)\nconvert_thm_fgm_to_parquet(probe, trange)"
  }
]