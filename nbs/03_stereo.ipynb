{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from STEREO\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "output-file: stereo.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPEDAS_DATA_DIR'] = f\"{os.environ['HOME']}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: import all the packages needed for the project\n",
    "#| output: hide\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "\n",
    "from ids_finder.utils import *\n",
    "from ids_finder.core import *\n",
    "\n",
    "import polars as pl\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "from loguru import logger\n",
    "import speasy as spz\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "import altair as alt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_probes = [\"a\", \"b\"]\n",
    "probe = stereo_probes[0]\n",
    "\n",
    "jno_start_date = \"2011-08-25\"\n",
    "jno_end_date = \"2016-06-30\" \n",
    "\n",
    "trange = [jno_start_date, jno_end_date]\n",
    "test_trange = [\"2011-08-25\", \"2012-08-26\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = 'STA'\n",
    "coord = 'rtn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-29 23:58:56.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 4>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSTEREO Ahead IMPACT/MAG Magnetic Field Vectors (RTN) - J. Luhmann (UCB/SSL)\u001b[0m\n",
      "\u001b[32m2023-09-29 23:58:56.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 5>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1msta_l1_mag_rtn_cdaweb\u001b[0m\n",
      "\u001b[32m2023-09-29 23:58:56.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 6>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mMagnetic field vector in RTN coordinates from the IMPACT/MAG instrument.\u001b[0m\n",
      "\u001b[32m2023-09-29 23:58:56.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 7>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mSTA_L1_MAG_RTN/BFIELD\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Magnetic field vector in RTN coordinates from the IMPACT/MAG instrument.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cda_tree: spz.SpeasyIndex = spz.inventories.tree.cda\n",
    "product = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN\n",
    "\n",
    "logger.info(product.description)\n",
    "logger.info(product.ID)\n",
    "logger.info(product.BFIELD.CATDESC)\n",
    "logger.info(product.BFIELD.spz_uid())\n",
    "\n",
    "# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.\n",
    "# spz.inventories.data_tree.cda.STEREO.STEREOA.IMPACT_MAG.STA_LB_MAG_RTN.description\n",
    "# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.MAGFLAGUC.CATDESC\n",
    "spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD.CATDESC\n",
    "# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the files\n",
    "\n",
    "Download data using `pyspedas`, but load it using `pycdfpp` (using `pyspedas` to load the data directly into `xarray` is very slow)\n",
    "\n",
    "Using `wget` does not work.\n",
    "\n",
    "`wget --recursive --no-parent --no-clobber http://sprg.ssl.berkeley.edu/data/misc/stereo/impact/level1/ahead/mag/RTN/2014`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspedas\n",
    "import pycdfpp\n",
    "from sunpy.time import TimeRange\n",
    "from pipe import select, take, where\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File size is too large to process at once, split it into multiple time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_df(trange, save=False):\n",
    "    if isinstance(trange, TimeRange):\n",
    "        trange = [trange.start.strftime(\"%Y-%m-%d\"), trange.end.strftime(\"%Y-%m-%d\")]\n",
    "\n",
    "    output = f\"../data/{sat}_data_{trange[0]}.parquet\"\n",
    "    if Path(output).exists():\n",
    "        logger.info(f\"Data exists. Reading {output}\")\n",
    "        return pl.read_parquet(output)\n",
    "\n",
    "    files = pyspedas.stereo.mag(trange, downloadonly=True)\n",
    "\n",
    "    cdfs = [pycdfpp.load(file) for file in files]\n",
    "    times = [pycdfpp.to_datetime64(cdf[\"Epoch\"]) for cdf in cdfs]\n",
    "    BFIELDs = [cdf[\"BFIELD\"].values for cdf in cdfs]\n",
    "\n",
    "    time = np.concatenate(times)\n",
    "    BFIELD = np.concatenate(BFIELDs)\n",
    "\n",
    "    df = pl.DataFrame(\n",
    "        {\n",
    "            \"time\": time,\n",
    "            \"BX\": BFIELD[:, 0],\n",
    "            \"BY\": BFIELD[:, 1],\n",
    "            \"BZ\": BFIELD[:, 2],\n",
    "            \"B\": BFIELD[:, 3],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        df.to_parquet(output)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_downsample(\n",
    "    data: pl.DataFrame, every: timedelta, period: timedelta\n",
    ") -> pl.DataFrame:\n",
    "    return (\n",
    "        data.sort(\"time\")\n",
    "        .group_by_dynamic(\"time\", every=every, period=period)\n",
    "        .agg(pl.col([\"BX\", \"BY\", \"BZ\", \"B\"]).mean())\n",
    "        .with_columns(pl.col(\"time\") + period / 2)\n",
    "        .with_columns(pl.col(\"time\").dt.cast_time_unit(\"ns\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-29 23:58:56.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 6>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mData exists. Reading ../data/STA_data_downsampled.parquet\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "every = timedelta(seconds=1)\n",
    "period = 2 * every\n",
    "\n",
    "output = f\"../data/{sat}_data_downsampled.parquet\"\n",
    "\n",
    "if Path(output).exists():\n",
    "    logger.info(f\"Data exists. Reading {output}\")\n",
    "else:\n",
    "    df_downsampled = pl.concat(\n",
    "        TimeRange(trange).split(10)\n",
    "        | select(st_df)\n",
    "        | select(lambda df: st_downsample(df, every, period))\n",
    "    )\n",
    "    df_downsampled.write_parquet(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: one can also use `speasy` to download data, however this is slower for `STEREO` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "sat_fgm_product = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD\n",
       "sat_fgm_product = 'cda/STA_L1_MAG_RTN/BFIELD'\n",
       "products = [sat_fgm_product]\n",
       "\n",
       "dataset = spz.get_data(products, test_trange, disable_proxy=True)\n",
       "sat_fgm_data  = dataset[0]\n",
       "data_preview(sat_fgm_data)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "sat_fgm_product = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD\n",
    "sat_fgm_product = 'cda/STA_L1_MAG_RTN/BFIELD'\n",
    "products = [sat_fgm_product]\n",
    "\n",
    "dataset = spz.get_data(products, test_trange, disable_proxy=True)\n",
    "sat_fgm_data  = dataset[0]\n",
    "data_preview(sat_fgm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data in a background thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "@threaded\n",
       "def download_data(products, trange):\n",
       "    logger.info(\"Downloading data\")\n",
       "    spz.get_data(products, trange, disable_proxy=True)\n",
       "    logger.info(\"Data downloaded\")\n",
       "    \n",
       "download_data(products, trange)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "\n",
    "@threaded\n",
    "def download_data(products, trange):\n",
    "    logger.info(\"Downloading data\")\n",
    "    spz.get_data(products, trange, disable_proxy=True)\n",
    "    logger.info(\"Data downloaded\")\n",
    "    \n",
    "download_data(products, trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and preprocess the data\n",
    "\n",
    "As we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when `X, SSE` and `X, GSE` is positive.\n",
    "\n",
    "- State data time resolution is 1 minute...\n",
    "\n",
    "- FGS data time resolution is 4 second..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "df = (\n",
       "    sat_state_sw.upsample(\"time\", every=\"1m\")\n",
       "    .group_by_dynamic(\"time\", every=\"1d\")\n",
       "    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n",
       "    .with_columns(\n",
       "        pl.when(pl.col(\"null_count\") > 720).then(0).otherwise(1).alias(\"availablity\")\n",
       "    )\n",
       ")\n",
       "\n",
       "properties = {\n",
       "    'width': 800,\n",
       "}\n",
       "\n",
       "chart1 = alt.Chart(df).mark_point().encode(\n",
       "    x='time',\n",
       "    y='null_count'\n",
       ").properties(**properties)\n",
       "\n",
       "chart2  = alt.Chart(df).mark_point().encode(\n",
       "    x='time',\n",
       "    y='availablity'\n",
       ").properties(**properties)\n",
       "\n",
       "(chart1 & chart2)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "df = (\n",
    "    sat_state_sw.upsample(\"time\", every=\"1m\")\n",
    "    .group_by_dynamic(\"time\", every=\"1d\")\n",
    "    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"null_count\") > 720).then(0).otherwise(1).alias(\"availablity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "properties = {\n",
    "    'width': 800,\n",
    "}\n",
    "\n",
    "chart1 = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='null_count'\n",
    ").properties(**properties)\n",
    "\n",
    "chart2  = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='availablity'\n",
    ").properties(**properties)\n",
    "\n",
    "(chart1 & chart2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "sat = \"sta\"\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "files = f\"../data/{sat}_data_downsampled.parquet\"\n",
    "output = f'../data/{sat}_candidates_tau_{tau.seconds}.parquet'\n",
    "\n",
    "data = pl.scan_parquet(files).collect()\n",
    "if data.get_column('time').is_sorted():\n",
    "    data = data.set_sorted('time')\n",
    "else:\n",
    "    data = data.sort('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = compute_indices(data, tau)\n",
    "\n",
    "# filter condition\n",
    "sparse_num = tau / data_resolution // 3\n",
    "filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "del indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that for missing data, fill values consisting of a blank followed \n",
    "by 9's which together constitute the format are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = \"\"\"Year\n",
    "DOY\n",
    "Hour\n",
    "Radial Distance, AU\n",
    "HGI Lat. of the S/C\n",
    "HGI Long. of the S/C\n",
    "IMF BR, nT (RTN)\n",
    "IMF BT, nT (RTN)\n",
    "IMF BN, nT (RTN)\n",
    "IMF B Scalar, nT\n",
    "SW Plasma Speed, km/s\n",
    "SW Lat. Angle RTN, deg.\n",
    "SW Long. Angle RTN, deg.\n",
    "SW Plasma Density, N/cm^3\n",
    "SW Plasma Temperature, K\n",
    "1.8-3.6 MeV H flux,LET\n",
    "4.0-6.0 MeV H flux,LET\n",
    "6.0-10.0 MeV H flux, LET\n",
    "10.0-12.0 MeV H flux,LET\n",
    "13.6-15.1 MeV H flux, HET\n",
    "14.9-17.1 MeV H flux, HET\n",
    "17.0-19.3 MeV H flux, HET\n",
    "20.8-23.8 MeV H flux, HET\n",
    "23.8-26.4 MeV H flux, HET\n",
    "26.3-29.7 MeV H flux, HET\n",
    "29.5-33.4 MeV H flux, HET\n",
    "33.4-35.8 MeV H flux, HET\n",
    "35.5-40.5 MeV H flux, HET\n",
    "40.0-60.0 MeV H flux, HET\n",
    "60.0-100.0 MeV H flux, HET\n",
    "0.320-0.452 MeV H flux, SIT\n",
    "0.452-0.64 MeV H flux, SIT\n",
    "0.640-0.905 MeV H flux, SIT\n",
    "0.905-1.28 MeV H flux, SIT\n",
    "1.280-1.81 MeV H flux, SIT\n",
    "1.810-2.56 MeV H flux, SIT\n",
    "2.560-3.62 MeV H flux, SIT\"\"\"\n",
    "\n",
    "def stereo_load_state(trange):\n",
    "    from fastdownload import FastDownload\n",
    "\n",
    "    d = FastDownload(base='../', archive='data', data='data')\n",
    "    \n",
    "    start_time = pd.Timestamp(trange[0])\n",
    "    end_time = pd.Timestamp(trange[1])\n",
    "    \n",
    "    url = \"https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/l2/merged/stereoa{year}.asc\"\n",
    "    columns = parameters.split(\"\\n\")\n",
    "    \n",
    "    df = pandas.concat(\n",
    "        range(start_time.year, end_time.year + 1)\n",
    "        | select(lambda x: url.format(year=x))\n",
    "        | select(d.download)\n",
    "        | select(lambda file: pandas.read_csv(file, delim_whitespace=True, names=columns)) # Read the file\n",
    "    )\n",
    "    \n",
    "    \n",
    "    data = pl.DataFrame(df).select(\n",
    "        pl.col(['Radial Distance, AU', 'HGI Lat. of the S/C', 'HGI Long. of the S/C']),\n",
    "        (pl.datetime(pl.col(\"Year\"), month=1, day=1)\n",
    "        + pl.duration(days=pl.col(\"DOY\") - 1, hours=pl.col(\"Hour\"))).dt.cast_time_unit(\"ns\").alias(\"time\"),\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-29 23:59:23.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mids_finder.utils\u001b[0m:\u001b[36mget_memory_usage\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1m2.8 GB (DataFrame)\u001b[0m\n",
      "\u001b[32m2023-09-29 23:59:28.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mids_finder.utils\u001b[0m:\u001b[36mget_memory_usage\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1m227.1 MB (DataArray)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage(data)\n",
    "sat_fgm = df2ts(\n",
    "    compress_data_by_cands(data, candidates), [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"}\n",
    ")\n",
    "get_memory_usage(sat_fgm)\n",
    "\n",
    "sat_state = stereo_load_state(trange)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Sep-23 23:59:44: UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "\n",
      "29-Sep-23 23:59:47: Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x28f6b5120>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "29-Sep-23 23:59:47: Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n",
      "2023-09-29 23:59:47,591\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "29-Sep-23 23:59:49: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Estimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "30-Sep-23 00:01:11: UserWarning: `DataFrame.<lambda>` for empty DataFrame is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "Please refer to https://modin.readthedocs.io/en/stable/supported_apis/defaulting_to_pandas.html for explanation.\n",
      "\n",
      "30-Sep-23 00:01:11: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "ids = process_candidates(candidates, sat_fgm, sat_state, data_resolution)\n",
    "ids = ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])\n",
    "ids.write_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "test_eq(ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).shape, ids.unique(\"d_time\").shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
