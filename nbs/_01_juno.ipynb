{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacecraft-Solar equatorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate System of Data:\n",
    "\n",
    "1. **SE (Solar Equatorial)**\n",
    "    - Code: `se`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `se_rN[N]s`\n",
    "        - Resampled 1 hour: `se_r1h`\n",
    "\n",
    "2. **PC (Planetocentric)**\n",
    "    - Code: `pc`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `pc_rN[N]s`\n",
    "        \n",
    "3. **SS (Sun-State)**\n",
    "    - Code: `ss`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `ss_rN[N]s`\n",
    "        \n",
    "4. **PL (Payload)**\n",
    "    - Code: `pl`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `pl_rN[N]s`\n",
    "\n",
    "\n",
    "```txt\n",
    "------------------------------------------------------------------------------\n",
    "Juno Mission Phases                                                           \n",
    "------------------------------------------------------------------------------\n",
    "Start       Mission                                                           \n",
    "Date        Phase                                                             \n",
    "==============================================================================\n",
    "2011-08-05  Launch                                                            \n",
    "2011-08-08  Inner Cruise 1                                                    \n",
    "2011-10-10  Inner Cruise 2                                                    \n",
    "2013-05-28  Inner Cruise 3                                                    \n",
    "2013-11-05  Quiet Cruise                                                      \n",
    "2016-01-05  Jupiter Approach                                                  \n",
    "2016-06-30  Jupiter Orbital Insertion                                         \n",
    "2016-07-05  Capture Orbit                                                     \n",
    "2016-10-19  Period Reduction Maneuver                                         \n",
    "2016-10-20  Orbits 1-2                                                        \n",
    "2016-11-09  Science Orbits                                                    \n",
    "2017-10-11  Deorbit\n",
    "```\n",
    "\n",
    "```txt\n",
    "File Naming Convention                                                        \n",
    "==============================================================================\n",
    "Convention:                                                                   \n",
    "   fgm_jno_LL_CCYYDDDxx_vVV.ext                                               \n",
    "Where:                                                                        \n",
    "   fgm - Fluxgate Magnetometer three character instrument abbreviation        \n",
    "   jno - Juno                                                                 \n",
    "    LL - CODMAC Data level, for example, l3 for level 3                       \n",
    "    CC - The century portion of a date, 20                                    \n",
    "    YY - The year of century portion of a date, 00-99                         \n",
    "   DDD - The day of year, 001-366                                             \n",
    "    xx - Coordinate system of data (se = Solar equatorial, ser = Solar        \n",
    "         equatorial resampled, pc = Planetocentric, ss = Sun-State,           \n",
    "         pl = Payload)                                                        \n",
    "     v - separator to denote Version number                                   \n",
    "    VV - version                                                              \n",
    "   ext - file extension (sts = Standard Time Series (ASCII) file, lbl = Label \n",
    "         file)                                                                \n",
    "Example:                                                                      \n",
    "   fgm_jno_l3_2014055se_v00.sts    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: import all the packages needed for the project\n",
    "\n",
    "from ids_finder.utils import *\n",
    "from ids_finder.core import *\n",
    "\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import pytplot\n",
    "from pytplot import timebar\n",
    "from pytplot import get_data, store_data, tplot, split_vec, join_vec, tplot_options, options, tlimit, highlight, degap\n",
    "\n",
    "import pdpipe as pdp\n",
    "\n",
    "\n",
    "from collections.abc import Callable\n",
    "from pandas import (\n",
    "    DataFrame,\n",
    "    Timestamp,\n",
    ")\n",
    "from xarray.core.dataarray import DataArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-Sep-23 10:46:01: UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "\n",
      "22-Sep-23 10:46:03: Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "22-Sep-23 10:46:03: Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n",
      "2023-09-22 10:46:03,886\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "22-Sep-23 10:46:05: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n"
     ]
    }
   ],
   "source": [
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2011\n",
    "    files = f'../data/{sat}_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "    sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "    sat_state = df2ts(data, [\"X\", \"Y\", \"Z\"], attrs={\"coordinate_system\": coord, \"units\": \"km\"})\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "    candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "    candidates = convert_to_dataframe(candidates_pl)\n",
    "    \n",
    "    ids = process_candidates(candidates, sat_fgm, sat_state, data_resolution)\n",
    "    \n",
    "    if isinstance(ids, mpd.DataFrame):\n",
    "        ids._to_pandas().to_parquet(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -r --no-parent --no-clobber 'https://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC/'\n",
    "# !aria2c -x 16 -s 16 'https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from `lbl` to `orc` format\n",
    "import os\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import pdr\n",
    "    \n",
    "def convert_file(file_path: Path, target_suffix: str, conversion_func: callable, check_exist = True) -> None:\n",
    "    target_file = file_path.with_suffix(target_suffix)\n",
    "    if check_exist and target_file.exists():\n",
    "        logger.info(f\"File {target_file} already exists. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    conversion_func(file_path, target_file)\n",
    "    logger.info(f\"Converted {file_path} to {target_file}\")\n",
    "\n",
    "def lbl_to_orc_conversion(src: Path, dest: Path) -> None:\n",
    "    df = pdr.read(src).TABLE\n",
    "    df.to_orc(dest)\n",
    "\n",
    "def orc_to_parquet_conversion(src: Path, dest: Path) -> None:\n",
    "    # We can also read partitioned datasets with multiple ORC files through the pyarrow.dataset interface.\n",
    "    \n",
    "    from pyarrow import orc\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    # import polars as pl\n",
    "    # df = pl.from_arrow( orc.read_table(src) )\n",
    "    # df.write_parquet(dest)\n",
    "    \n",
    "    table = orc.read_table(src)\n",
    "    pq.write_table(table, dest)\n",
    "\n",
    "def convert_format(format_from, format_to):\n",
    "    conversion_map = {\n",
    "        ('lbl', 'orc'): lbl_to_orc_conversion,\n",
    "        ('orc', 'parquet'): orc_to_parquet_conversion\n",
    "    }\n",
    "    \n",
    "    convert_func = conversion_map.get((format_from, format_to))\n",
    "    if not convert_func:\n",
    "        raise ValueError(f\"Conversion from {format_from} to {format_to} is not supported\")\n",
    "\n",
    "    local_dir = Path(os.environ['HOME']) / 'juno'\n",
    "    pattern = f'**/*.{format_from}'\n",
    "\n",
    "    for file in local_dir.glob(pattern):\n",
    "        convert_file(file, f\".{format_to}\", convert_func)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # convert_format('lbl', 'orc')\n",
    "    convert_format('orc', 'parquet')\n",
    "\n",
    "\n",
    "# delete all files with extension\n",
    "# find . -type f -name '*.parquet' -delete\n",
    "# find . -type f -name '*.orc' -delete\n",
    "# find . -type f -name '*.lbl' -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_dir = \"https://pds-ppi.igpp.ucla.edu/data\"\n",
    "\n",
    "possible_coords = [\"se\", \"ser\", \"pc\", \"ss\", \"pl\"]\n",
    "possible_exts = [\"sts\", \"lbl\"]\n",
    "possible_data_rates = [\"1s\", \"1min\", \"1h\"]\n",
    "\n",
    "juno_ss_config = {\n",
    "    \"DATA_SET_ID\": \"JNO-SS-3-FGM-CAL-V1.0\",\n",
    "    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n",
    "}\n",
    "\n",
    "juno_j_config = {\n",
    "    \"DATA_SET_ID\": \"JNO-J-3-FGM-CAL-V1.0\",\n",
    "    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdr\n",
    "\n",
    "def download_and_read_file(config, index_table=False):\n",
    "    \"\"\"Download and read file for each config.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The data read from the file.\n",
    "    \"\"\"\n",
    "    # BUG: index file is not formatted properly according to `lbl` file, so can not be used with `pdr` for.\n",
    "    # ValueError: time data \"282T00:00:31.130,2019\" doesn't match format \"%Y-%jT%H:%M:%S.%f\", at position 3553. You might want to try:\n",
    "    # - passing `format` if your strings have a consistent format;\n",
    "    # - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
    "    # - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
    "\n",
    "    local_dir = os.path.join(os.environ[\"HOME\"], \"juno\", config[\"DATA_SET_ID\"])\n",
    "    base_url = f\"{pds_dir}/{config['DATA_SET_ID']}\"\n",
    "\n",
    "    lbl_fn = config[\"FILE_SPECIFICATION_NAME\"]\n",
    "\n",
    "    if not index_table:\n",
    "        parquet_fn = lbl_fn.replace(\"lbl\", \"parquet\")\n",
    "        parquet_fp = os.path.join(local_dir, parquet_fn)\n",
    "        if os.path.exists(parquet_fp):\n",
    "            return pandas.read_parquet(os.path.join(local_dir, parquet_fn))\n",
    "\n",
    "    lbl_file_url = f\"{base_url}/{lbl_fn}\"\n",
    "    lbl_fp = download_file(lbl_file_url, local_dir, lbl_fn)\n",
    "    logger.debug(f\"Reading {lbl_fp}\")\n",
    "\n",
    "    if index_table:\n",
    "        tab_fn = lbl_fn.replace(\"LBL\", \"TAB\")\n",
    "        tab_fp = download_file(f\"{base_url}/{tab_fn}\", local_dir, tab_fn)\n",
    "        tab_index = pandas.read_csv(tab_fp, delimiter=\",\", quotechar='\"')\n",
    "        tab_index.columns = tab_index.columns.str.replace(\" \", \"\")\n",
    "        return tab_index\n",
    "    else:\n",
    "        sts_fn = lbl_fn.replace(\"lbl\", \"sts\")\n",
    "        download_file(f\"{base_url}/{sts_fn}\", local_dir, sts_fn)\n",
    "        return pdr.read(lbl_fp).TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-21 02:19:27.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdownload_and_read_file\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mReading /Users/zijin/juno/JNO-SS-3-FGM-CAL-V1.0/INDEX/INDEX.LBL\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdownload_and_read_file\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mReading /Users/zijin/juno/JNO-J-3-FGM-CAL-V1.0/INDEX/INDEX.LBL\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "juno_ss_index = download_and_read_file(juno_ss_config, index_table=True)\n",
    "juno_j_index = download_and_read_file(juno_j_config, index_table=True)\n",
    "\n",
    "_index_time_format = \"%Y-%jT%H:%M:%S.%f\"\n",
    "\n",
    "jno_pipeline = pdp.PdPipeline(\n",
    "    [\n",
    "        pdp.ColDrop([\"PRODUCT_ID\", \"CR_DATE\", \"PRODUCT_LABEL_MD5CHECKSUM\"]),\n",
    "        pdp.ApplyByCols(\"SID\", str.rstrip),\n",
    "        pdp.ApplyByCols(\"FILE_SPECIFICATION_NAME\", str.rstrip),\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"START_TIME\",\n",
    "            lambda df: pandas.to_datetime(df[\"START_TIME\"], format=_index_time_format),\n",
    "        ),\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"STOP_TIME\",\n",
    "            lambda df: pandas.to_datetime(df[\"STOP_TIME\"], format=_index_time_format),\n",
    "        ),\n",
    "        # pdp.ApplyByCols(['START_TIME', 'STOP_TIME'], pandas.to_datetime, format=_index_time_format), # NOTE: This is slow\n",
    "    ]\n",
    ")\n",
    "if True:\n",
    "    index_df = pandas.concat(\n",
    "        [jno_pipeline(juno_ss_index), jno_pipeline(juno_j_index)], ignore_index=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juno_load_fgm(trange: list, coord=\"se\", data_rate=\"1s\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get the data array for a given time range and coordinate.\n",
    "\n",
    "    Parameters:\n",
    "        trange (list): The time range.\n",
    "        coord (str, optional): The coordinate. Defaults to 'se'.\n",
    "        data_rate (str, optional): The data rate. Defaults to '1s'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The dataframe for the given time range and coordinate.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(trange) != 2:\n",
    "        raise ValueError(\n",
    "            \"Expected trange to have exactly 2 elements: start and stop time.\"\n",
    "        )\n",
    "\n",
    "    start_time = pandas.Timestamp(trange[0])\n",
    "    stop_time = pandas.Timestamp(trange[1])\n",
    "\n",
    "    temp_index_df = index_df[\n",
    "        (index_df[\"SID\"] == get_sid(coord, data_rate))\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # Filtering\n",
    "    relevant_files = temp_index_df[\n",
    "        (temp_index_df[\"STOP_TIME\"] > start_time)\n",
    "        & (temp_index_df[\"START_TIME\"] < stop_time)\n",
    "    ]\n",
    "    dataframes = [download_and_read_file(row) for _, row in relevant_files.iterrows()]\n",
    "\n",
    "    # rows = [row for _, row in relevant_files.iterrows()]\n",
    "    # with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    #     dataframes = list(executor.map(download_and_read_file, rows))\n",
    "\n",
    "    combined_data = pandas.concat(dataframes)\n",
    "\n",
    "    return pdp_process_juno_df(combined_data)\n",
    "\n",
    "def get_sid(coord, data_rate):\n",
    "    sid_mapping = {\n",
    "        \"pc\": {\"1s\": \"PC 1 SECOND\", \"1min\": \"PC 1 MINUTE\", \"\": \"PCENTRIC\"},\n",
    "        \"pl\": {\"1s\": \"PAYLOAD 1 SECOND\", \"\": \"PAYLOAD\"},\n",
    "        \"ss\": {\"1s\": \"SS 1 SECOND\", \"1min\": \"SS 1 MINUTE\", \"\": \"SUNSTATE\"},\n",
    "        \"se\": {\"1s\": \"SE 1 SECOND\", \"1min\": \"SE 1 MINUTE\", \"\": \"SE\"},\n",
    "    }\n",
    "    try:\n",
    "        return sid_mapping[coord][data_rate]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "_skip_cond = ~pdp.cond.HasAllColumns([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"])\n",
    "pdp_process_juno_df = pdp.PdPipeline(\n",
    "    [\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"time\",\n",
    "            lambda df: pandas.to_datetime(df[\"SAMPLE UTC\"], format=\"%Y %j %H %M %S %f\"),\n",
    "            skip=_skip_cond,\n",
    "        ),\n",
    "        pdp.ColDrop([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"], skip=_skip_cond),\n",
    "        pdp.df.set_index(\"time\"),\n",
    "        pdp.ColRename(col_renamer)\n",
    "        # pdp.AggByCols('SAMPLE UTC', func=lambda time: pandas.to_datetime(time, format='%Y %j %H %M %S %f'), func_desc='Convert time to datetime') # NOTE: this is quite slow\n",
    "        # pdp.df['time'] << pandas.to_datetime(pdp.df['SAMPLE UTC'], format='%Y %j %H %M %S %f'), # NOTE: this is not work\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 day of data resampled by 1 sec is about 12 MB.\n",
    "\n",
    "So 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\n",
    "\n",
    "Downloading rate is about 250 KB/s, so it will take about 3 days to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to download: 126.53 hours\n",
      "Disk space required: 113.88 GB\n",
      "Time to process: 36.50 hours\n"
     ]
    }
   ],
   "source": [
    "num_of_files = 6*365\n",
    "jno_file_size = 12e3\n",
    "thm_file_size = 40e3\n",
    "files_size = jno_file_size + thm_file_size\n",
    "downloading_rate = 250\n",
    "processing_rate = 1/60\n",
    "\n",
    "time_to_download = num_of_files * files_size / downloading_rate / 60 / 60\n",
    "space_required = num_of_files * files_size / 1e6\n",
    "time_to_process = num_of_files / processing_rate / 60 / 60\n",
    "\n",
    "print(f\"Time to download: {time_to_download:.2f} hours\")\n",
    "print(f\"Disk space required: {space_required:.2f} GB\")\n",
    "print(f\"Time to process: {time_to_process:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data format\n",
    "\n",
    "See [convert_format.py](convert_format.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-21 02:19:27.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 6>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mStarting date: 2011-08-25\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 7>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mEnding date: 2016-06-29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jno_ss_index_df = index_df.loc[ lambda _: _['VOLUME_ID'] == 'JNOFGM_1000']\n",
    "\n",
    "starting_date = jno_ss_index_df['START_TIME'].min().date()\n",
    "ending_date = jno_ss_index_df['STOP_TIME'].max().date()\n",
    "\n",
    "logger.info(f\"Starting date: {starting_date}\")\n",
    "logger.info(f\"Ending date: {ending_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following days are missing\n",
      "2012-04-20\n",
      "2012-04-21\n",
      "2012-04-22\n",
      "2012-04-23\n",
      "2012-04-24\n",
      "2012-05-15\n",
      "2012-06-15\n",
      "2012-07-04\n",
      "2012-07-05\n",
      "2012-07-06\n",
      "2012-07-07\n",
      "2012-07-08\n",
      "2012-08-25\n",
      "2012-08-26\n",
      "2012-08-27\n",
      "2012-08-28\n",
      "2012-08-29\n",
      "2012-08-30\n",
      "2012-08-31\n",
      "2012-09-01\n",
      "2012-09-02\n",
      "2012-09-03\n",
      "2012-09-04\n",
      "2012-09-05\n",
      "2012-09-06\n",
      "2012-09-07\n",
      "2012-09-08\n",
      "2012-09-09\n",
      "2012-09-10\n",
      "2012-09-11\n",
      "2012-09-12\n",
      "2012-09-13\n",
      "2012-09-14\n",
      "2012-09-15\n",
      "2012-09-16\n",
      "2012-09-17\n",
      "2012-10-05\n",
      "2012-10-06\n",
      "2012-10-07\n",
      "2012-10-08\n",
      "2012-12-13\n",
      "2012-12-14\n",
      "2012-12-15\n",
      "2012-12-16\n",
      "2012-12-17\n",
      "2012-12-18\n",
      "2013-05-11\n",
      "2013-05-12\n",
      "2013-05-13\n",
      "2013-05-14\n",
      "2013-05-23\n",
      "2013-05-24\n",
      "2013-05-25\n",
      "2013-05-26\n",
      "2013-05-27\n",
      "2013-05-28\n",
      "2013-05-29\n",
      "2013-05-30\n",
      "2013-05-31\n",
      "2013-06-01\n",
      "2013-06-02\n",
      "2013-06-03\n",
      "2013-06-04\n",
      "2013-06-05\n",
      "2013-06-06\n",
      "2013-06-07\n",
      "2013-06-08\n",
      "2013-06-09\n",
      "2013-06-10\n",
      "2013-06-11\n",
      "2013-06-12\n",
      "2013-06-13\n",
      "2013-06-14\n",
      "2013-06-15\n",
      "2013-06-16\n",
      "2013-06-17\n",
      "2013-06-18\n",
      "2013-06-19\n",
      "2013-06-20\n",
      "2013-06-21\n",
      "2013-06-22\n",
      "2013-06-23\n",
      "2013-06-24\n",
      "2013-06-25\n",
      "2013-06-26\n",
      "2013-06-27\n",
      "2013-06-28\n",
      "2013-06-29\n",
      "2013-06-30\n",
      "2013-07-01\n",
      "2013-07-03\n",
      "2013-07-04\n",
      "2013-07-05\n",
      "2013-07-06\n",
      "2013-07-07\n",
      "2013-07-08\n",
      "2013-07-09\n",
      "2013-07-10\n",
      "2013-07-11\n",
      "2013-07-12\n",
      "2013-07-13\n",
      "2013-07-14\n",
      "2013-07-15\n",
      "2013-07-16\n",
      "2013-07-17\n",
      "2013-07-18\n",
      "2013-07-19\n",
      "2013-07-20\n",
      "2013-07-21\n",
      "2013-07-22\n",
      "2013-07-23\n",
      "2013-07-24\n",
      "2013-07-25\n",
      "2013-07-26\n",
      "2013-07-27\n",
      "2013-07-28\n",
      "2013-07-29\n",
      "2013-07-30\n",
      "2013-07-31\n",
      "2013-08-01\n",
      "2013-08-02\n",
      "2013-08-03\n",
      "2013-08-04\n",
      "2013-08-05\n",
      "2013-08-06\n",
      "2013-08-07\n",
      "2013-08-08\n",
      "2013-08-09\n",
      "2013-08-10\n",
      "2013-08-11\n",
      "2013-08-12\n",
      "2013-08-13\n",
      "2013-08-14\n",
      "2013-08-15\n",
      "2013-08-16\n",
      "2013-08-17\n",
      "2013-08-18\n",
      "2013-08-19\n",
      "2013-08-20\n",
      "2013-08-21\n",
      "2013-08-22\n",
      "2013-08-23\n",
      "2013-08-24\n",
      "2013-08-25\n",
      "2013-08-26\n",
      "2013-08-27\n",
      "2013-08-28\n",
      "2013-08-29\n",
      "2013-08-30\n",
      "2013-08-31\n",
      "2013-09-01\n",
      "2013-09-02\n",
      "2013-09-03\n",
      "2013-09-04\n",
      "2013-09-05\n",
      "2013-09-06\n",
      "2013-09-07\n",
      "2013-09-08\n",
      "2013-09-09\n",
      "2013-09-10\n",
      "2013-09-11\n",
      "2013-09-12\n",
      "2013-09-13\n",
      "2013-09-14\n",
      "2013-09-15\n",
      "2013-09-16\n",
      "2013-09-17\n",
      "2013-09-18\n",
      "2013-09-19\n",
      "2013-09-20\n",
      "2013-09-21\n",
      "2013-09-22\n",
      "2013-09-23\n",
      "2013-09-24\n",
      "2013-09-25\n",
      "2013-09-26\n",
      "2013-09-27\n",
      "2013-09-28\n",
      "2013-09-29\n",
      "2013-09-30\n",
      "2013-10-01\n",
      "2013-10-02\n",
      "2013-10-03\n",
      "2013-10-04\n",
      "2013-10-10\n",
      "2013-10-11\n",
      "2013-10-12\n",
      "2013-10-13\n",
      "2013-10-14\n",
      "2013-10-15\n",
      "2013-10-16\n",
      "2013-10-17\n",
      "2013-10-18\n",
      "2013-10-19\n",
      "2013-10-20\n",
      "2013-10-21\n",
      "2013-10-22\n",
      "2014-03-14\n",
      "2014-03-15\n",
      "2014-03-16\n",
      "2014-03-17\n",
      "2014-03-18\n",
      "2014-03-19\n",
      "2014-03-20\n",
      "2014-03-21\n",
      "2014-03-22\n",
      "2014-03-23\n",
      "2014-03-24\n",
      "2014-03-25\n",
      "2014-03-26\n",
      "2014-03-27\n",
      "2014-03-28\n",
      "2014-03-29\n",
      "2014-03-30\n",
      "2014-03-31\n",
      "2014-04-01\n",
      "2014-04-02\n",
      "2014-07-12\n",
      "2014-07-13\n",
      "2014-08-09\n",
      "2014-08-10\n",
      "2014-09-06\n",
      "2014-09-09\n",
      "2014-09-10\n",
      "2014-09-11\n",
      "2014-09-12\n",
      "2014-09-13\n",
      "2014-09-14\n",
      "2014-10-07\n",
      "2014-10-08\n",
      "2014-10-09\n",
      "2014-10-10\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-11-29\n",
      "2014-11-30\n",
      "2014-12-06\n",
      "2014-12-07\n",
      "2014-12-28\n",
      "2014-12-31\n",
      "2015-08-03\n",
      "2015-08-04\n",
      "2015-08-05\n",
      "2015-08-06\n",
      "2015-11-06\n",
      "2015-11-07\n",
      "2015-12-05\n",
      "2015-12-06\n",
      "2015-12-07\n",
      "2015-12-08\n",
      "2015-12-09\n"
     ]
    }
   ],
   "source": [
    "available_dates = pandas.concat([jno_ss_index_df['START_TIME'].dt.date, jno_ss_index_df['STOP_TIME'].dt.date]).unique()\n",
    "full_year_range = pandas.date_range(start=starting_date, end=ending_date)\n",
    "\n",
    "missing_dates = full_year_range[~full_year_range.isin(available_dates)]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(f\"No days are missing.\")\n",
    "else:\n",
    "    print(f\"The following days are missing\")\n",
    "    for date in missing_dates:\n",
    "        print(date.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pre_process(year, force=False):\n",
    "\n",
    "    trange = [f\"{year}-01-01\", f\"{year+1}-01-02\"]  # having some overlap\n",
    "    dir_path = Path(os.environ[\"HOME\"], \"juno/JNO-SS-3-FGM-CAL-V1.0/\")\n",
    "    pattern = \"**/*.parquet\"\n",
    "    data = dir_path / pattern\n",
    "    \n",
    "    file = f\"data/jno_{year}.parquet\"\n",
    "    if os.path.exists(file) and not force:\n",
    "        logger.info(f\"File {file} exists. Skipping\")\n",
    "        return file\n",
    "    logger.info(f\"Preprocessing data for year {year}\")\n",
    "    \n",
    "    lazy_df = pl.scan_parquet(data)\n",
    "    temp_df = (\n",
    "        lazy_df.filter(\n",
    "            pl.col(\"time\").is_between(pd.Timestamp(trange[0]), pd.Timestamp(trange[1])),\n",
    "        )\n",
    "        .sort(\n",
    "            \"time\"\n",
    "        )  # needed for `compute_index_std` to work properly as `group_by_dynamic` requires the data to be sorted\n",
    "        .filter(\n",
    "            pl.col(\n",
    "                \"time\"\n",
    "            ).is_first_distinct()  # remove duplicate time values for xarray to select data properly, though significantly slows down the computation\n",
    "        )\n",
    "        .rename({\"BX SE\": \"BX\", \"BY SE\": \"BY\", \"BZ SE\": \"BZ\"})\n",
    "    )\n",
    "    temp_df.collect().write_parquet(file)\n",
    "    return file\n",
    "\n",
    "starting_year = starting_date.year\n",
    "ending_year = ending_date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-21 02:19:27.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2011.parquet exists. Skipping\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2012.parquet exists. Skipping\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2013.parquet exists. Skipping\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2014.parquet exists. Skipping\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2015.parquet exists. Skipping\u001b[0m\n",
      "\u001b[32m2023-09-21 02:19:27.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbatch_pre_process\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mFile data/jno_2016.parquet exists. Skipping\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for year in range(starting_year, ending_year+1):\n",
    "    batch_pre_process(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    trange = ['2012','2012-6-01']\n",
    "    trange = ['2016','2017']\n",
    "    trange = ['2011','2013']\n",
    "\n",
    "    coord = 'se'\n",
    "    data_rate='1s'\n",
    "    tau = 30\n",
    "    \n",
    "    # get a temporary dataframe for testing\n",
    "    dir_path = Path(os.environ[\"HOME\"], \"juno/JNO-SS-3-FGM-CAL-V1.0/\")\n",
    "    pattern = \"**/*.parquet\"\n",
    "    data = dir_path / pattern\n",
    "\n",
    "    lazy_df = pl.scan_parquet(data)\n",
    "    temp_df = (\n",
    "        lazy_df.filter(\n",
    "            pl.col(\"time\").is_between(pd.Timestamp(trange[0]), pd.Timestamp(trange[1])),\n",
    "        )\n",
    "        .sort(\"time\")\n",
    "        .rename({\"BX SE\": \"BX\", \"BY SE\": \"BY\", \"BZ SE\": \"BZ\"})\n",
    "    )\n",
    "\n",
    "    lazy = False\n",
    "    lazy = True\n",
    "    if not lazy:\n",
    "        temp_df = temp_df.collect()\n",
    "        # juno_fgm_df = juno_load_fgm(trange, coord=coord, data_rate=data_rate)\n",
    "        sat_fgm = sat_get_fgm_from_df(temp_df)\n",
    "        sat_state = juno_get_state(temp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first index is $$ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} $$\n",
    "The second index is $$ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} $$\n",
    "The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\n",
    "\n",
    "third index (relative field jump) is $$ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} $$ a supplementary condition toreduce the uncertainty of recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: get the number of data points in each time interval and inspect the result\n",
    "if test:\n",
    "    index_num = (\n",
    "        temp_df.group_by_dynamic(\n",
    "            \"time\",\n",
    "            every=f\"{tau//2}s\",\n",
    "            period=f\"{tau}s\",\n",
    "        )\n",
    "        .agg(pl.count())\n",
    "    )\n",
    "\n",
    "    sparse_num = tau // 4\n",
    "    sparse_intervals = index_num.filter(pl.col(\"count\") < sparse_num)\n",
    "    # logger.info(f'Num of intervals where data are sparse: {len(sparse_intervals)} out of {len(data_points_df)} ({len(sparse_intervals)/len(data_points_df)*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_index_std in module utils:\n",
      "\n",
      "compute_index_std(data, tau)\n",
      "    helper function to compute standard deviation index\n"
     ]
    }
   ],
   "source": [
    "help(compute_index_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_index_fluctuation in module utils:\n",
      "\n",
      "compute_index_fluctuation(data, tau)\n",
      "    helper function to compute fluctuation index\n",
      "    \n",
      "    Notes: the results returned are a little bit different for the two implementations (because of the implementation of `std`).\n",
      "\n",
      "Help on function compute_index_fluctuation_xr in module utils:\n",
      "\n",
      "compute_index_fluctuation_xr(data: xarray.core.dataarray.DataArray, tau: int) -> xarray.core.dataarray.DataArray\n",
      "    Computes the fluctuation index for a given data array based on a specified time interval.\n",
      "    \n",
      "    Parameters:\n",
      "    - data: The xarray DataArray containing the data to be processed.\n",
      "    - tau: Time interval in seconds for resampling.\n",
      "    \n",
      "    Returns:\n",
      "    - fluctuation: xarray DataArray containing the fluctuation indices.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        ddof=0 is used for calculating the standard deviation. (ddof=1 is for sample standard deviation)\n"
     ]
    }
   ],
   "source": [
    "help(compute_index_fluctuation)\n",
    "help(compute_index_fluctuation_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i2 = index_fluctuation(juno_fgm_b, tau)\n",
    "# index_fluctuation_df = compute_index_fluctuation(temp_df, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the relative field jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_diff(data: DataArray, tau):\n",
    "    grouped_data = data.resample(time=pd.Timedelta(tau, unit='s'))\n",
    "\n",
    "    dvecs = grouped_data.first()-grouped_data.last()\n",
    "    vec_mean_mags = grouped_data.map(calc_vec_mean_mag)\n",
    "    vec_diffs = linalg.norm(dvecs, dims='v_dim') / vec_mean_mags\n",
    "    \n",
    "    # vec_diffs = grouped_data.map(calc_vec_relative_diff) # NOTE: this is slower than the above implementation.\n",
    "    # INFO: Do your spatial and temporal indexing (e.g. .sel() or .isel()) early in the pipeline, especially before calling resample() or groupby(). Grouping and resampling triggers some computation on all the blocks, which in theory should commute with indexing, but this optimization hasn’t been implemented in Dask yet. (See Dask issue #746).\n",
    "    \n",
    "    offset = pd.Timedelta(tau/2, unit='s')\n",
    "    vec_diffs['time'] = vec_diffs['time'] + offset\n",
    "    return vec_diffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_data_xr(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    duration = candidate['tstop'] - candidate['tstart']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    return data.sel(time=slice(temp_tstart,  temp_tstop))\n",
    "\n",
    "def get_candidate_data_pl(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    much slower than `get_candidate_data_xr`\n",
    "    \"\"\"\n",
    "    duration = candidate['tstart'] - candidate['tstop']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    temp_data = data.filter(\n",
    "        pl.col(\"time\").is_between(temp_tstart, temp_tstop)\n",
    "    )\n",
    "    \n",
    "    dims = [\"v_dim\", \"time\"]\n",
    "    coords = {\n",
    "        \"time\": temp_data['time'], \n",
    "        \"v_dim\": [\"BX\", \"BY\", \"BZ\"]\n",
    "        }\n",
    "    return xr.DataArray([ temp_data['BX'], temp_data['BY'], temp_data['BZ']], dims=dims, coords=coords)\n",
    "\n",
    "def get_candidate_data(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        return get_candidate_data_xr(candidate, data, coord=coord, neighbor=neighbor)\n",
    "    elif isinstance(data, pl.DataFrame):    \n",
    "        return get_candidate_data_pl(candidate, data, coord=coord, neighbor=neighbor)\n",
    "\n",
    "def get_candidates(candidates: DataFrame, candidate_type=None, num:int=4):\n",
    "    \n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates['type'] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "    \n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\")\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspedas.cotrans import minvar_matrix_make\n",
    "from pyspedas import tvector_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basic(\n",
    "    data, tstart, tstop, tau, mva_tstart=None, mva_tstop=None, neighbor: int = 1\n",
    "):\n",
    "    if mva_tstart is None:\n",
    "        mva_tstart = tstart\n",
    "    if mva_tstop is None:\n",
    "        mva_tstop = tstop\n",
    "\n",
    "    mva_b = data.sel(time=slice(mva_tstart, mva_tstop))\n",
    "    store_data(\"fgm\", data={\"x\": mva_b.time, \"y\": mva_b})\n",
    "    minvar_matrix_make(\"fgm\")  # get the MVA matrix\n",
    "\n",
    "    temp_tstart = pd.Timestamp(tstart) - pd.Timedelta(neighbor * tau, unit=\"s\")\n",
    "    temp_tstop = pd.Timestamp(tstop) + pd.Timedelta(neighbor * tau, unit=\"s\")\n",
    "\n",
    "    temp_b = data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "    store_data(\"fgm\", data={\"x\": temp_b.time, \"y\": temp_b})\n",
    "    temp_btotal = calc_vec_mag(temp_b)\n",
    "    store_data(\"fgm_btotal\", data={\"x\": temp_btotal.time, \"y\": temp_btotal})\n",
    "\n",
    "    tvector_rotate(\"fgm_mva_mat\", \"fgm\")\n",
    "    split_vec(\"fgm_rot\")\n",
    "    pytplot.data_quants[\"fgm_btotal\"][\"time\"] = pytplot.data_quants[\"fgm_rot\"][\n",
    "        \"time\"\n",
    "    ]  # NOTE: whenever using `get_data`, we may lose precision in the time values. This is a workaround.\n",
    "    join_vec(\n",
    "        [\n",
    "            \"fgm_rot_x\",\n",
    "            \"fgm_rot_y\",\n",
    "            \"fgm_rot_z\",\n",
    "            \"fgm_btotal\",\n",
    "        ],\n",
    "        new_tvar=\"fgm_all\",\n",
    "    )\n",
    "\n",
    "    options(\"fgm\", \"legend_names\", [r\"$B_x$\", r\"$B_y$\", r\"$B_z$\"])\n",
    "    options(\"fgm_all\", \"legend_names\", [r\"$B_l$\", r\"$B_m$\", r\"$B_n$\", r\"$B_{total}$\"])\n",
    "    options(\"fgm_all\", \"ysubtitle\", \"[nT LMN]\")\n",
    "    highlight([\"fgm\", \"fgm_all\"], [tstart.timestamp(), tstop.timestamp()])\n",
    "    degap(\"fgm\")\n",
    "    degap(\"fgm_all\")\n",
    "\n",
    "def format_candidate_title(candidate: pandas.Series):\n",
    "    format_float = lambda x: rf\"$\\bf {x:.2f} $\" if isinstance(x, (float, int)) else rf\"$\\bf {x} $\"\n",
    "\n",
    "    base_line = rf'$\\bf {candidate.get(\"type\", \"N/A\")} $ candidate (time: {candidate.get(\"time\", \"N/A\")}) with index '\n",
    "    index_line = rf'i1: {format_float(candidate.get(\"index_std\", \"N/A\"))}, i2: {format_float(candidate.get(\"index_fluctuation\", \"N/A\"))}, i3: {format_float(candidate.get(\"index_diff\", \"N/A\"))}'\n",
    "    info_line = rf'$B_n/B$: {format_float(candidate.get(\"BnOverB\", \"N/A\"))}, $dB/B$: {format_float(candidate.get(\"dBOverB\", \"N/A\"))}, $(dB/B)_{{max}}$: {format_float(candidate.get(\"dBOverB_max\", \"N/A\"))},  $Q_{{mva}}$: {format_float(candidate.get(\"Q_mva\", \"N/A\"))}'\n",
    "    title = rf\"\"\"{base_line}\n",
    "    {index_line}\n",
    "    {info_line}\"\"\"\n",
    "    return title\n",
    "\n",
    "\n",
    "def plot_candidate(candidate: pandas.Series):\n",
    "    if pd.notnull(candidate.get(\"d_tstart\")) and pd.notnull(candidate.get(\"d_tstop\")):\n",
    "        plot_basic(\n",
    "            sat_fgm,\n",
    "            candidate[\"tstart\"],\n",
    "            candidate[\"tstop\"],\n",
    "            tau,\n",
    "            candidate[\"d_tstart\"],\n",
    "            candidate[\"d_tstop\"],\n",
    "        )\n",
    "    else:\n",
    "        plot_basic(sat_fgm, candidate[\"tstart\"], candidate[\"tstop\"], tau)\n",
    "\n",
    "    tplot_options(\"title\", format_candidate_title(candidate))\n",
    "\n",
    "    if \"d_time\" in candidate.keys():\n",
    "        timebar(candidate[\"d_time\"].timestamp(), color=\"red\")\n",
    "    if \"d_tstart\" in candidate.keys() and not pd.isnull(candidate[\"d_tstart\"]):\n",
    "        timebar(candidate[\"d_tstart\"].timestamp())\n",
    "    if \"d_tstop\" in candidate.keys() and not pd.isnull(candidate[\"d_tstop\"]):\n",
    "        timebar(candidate[\"d_tstop\"].timestamp())\n",
    "\n",
    "    # tplot(['fgm','fgm_all'])\n",
    "    tplot(\"fgm_all\")\n",
    "\n",
    "\n",
    "def plot_candidates(\n",
    "    candidates: pandas.DataFrame, candidate_type=None, num=4, plot_func=plot_candidate\n",
    "):\n",
    "    \"\"\"Plot a set of candidates.\n",
    "\n",
    "    Parameters:\n",
    "    - candidates (pd.DataFrame): DataFrame containing the candidates.\n",
    "    - candidate_type (str, optional): Filter candidates based on a specific type.\n",
    "    - num (int): Number of candidates to plot, selected randomly.\n",
    "    - plot_func (callable): Function used to plot an individual candidate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter by candidate_type if provided\n",
    "    candidates = get_candidates(candidates, candidate_type, num)\n",
    "\n",
    "    # Plot each candidate using the provided plotting function\n",
    "    for _, candidate in candidates.iterrows():\n",
    "        plot_func(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single candidate test\n",
    "if test:\n",
    "    temp_candidate = candidates.iloc[1].to_dict()\n",
    "    plot_candidate(temp_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of duration\n",
    "- Define $d^* = \\max( | dB / dt | ) $, and then define time interval where $| dB/dt |$ decreases to $d^*/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_RATIO  = 1/4\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def calc_duration(vec: xr.DataArray, threshold_ratio=THRESHOLD_RATIO) -> pandas.Series:\n",
    "    # NOTE: gradient calculated at the edge is not reliable.\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\").isel(time=slice(1,-1))\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    # Determine d_star based on trend\n",
    "    if vec_diff_mag.isnull().all():\n",
    "        raise ValueError(\"The differentiated vector magnitude contains only NaN values. Cannot compute duration.\")\n",
    "    \n",
    "    d_star_index = vec_diff_mag.argmax(dim=\"time\")\n",
    "    d_star = vec_diff_mag[d_star_index]\n",
    "    d_time = vec_diff_mag.time[d_star_index]\n",
    "    \n",
    "    threshold = d_star * threshold_ratio\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_star': d_star.item(),\n",
    "        'd_time': d_time.values,\n",
    "        'threshold': threshold.item(),\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    "\n",
    "def calc_d_duration(vec: xr.DataArray, d_time, threshold) -> pd.Series:\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    " \n",
    "def find_start_end_times(vec_diff_mag: xr.DataArray, d_time, threshold) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    # Determine start time\n",
    "    pre_vec_mag = vec_diff_mag.sel(time=slice(None, d_time))\n",
    "    start_time = get_time_from_condition(pre_vec_mag, threshold, \"last_below\")\n",
    "\n",
    "    # Determine stop time\n",
    "    post_vec_mag = vec_diff_mag.sel(time=slice(d_time, None))\n",
    "    end_time = get_time_from_condition(post_vec_mag, threshold, \"first_below\")\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def get_time_from_condition(vec: xr.DataArray, threshold, condition_type) -> pd.Timestamp:\n",
    "    if condition_type == \"first_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = 0\n",
    "    elif condition_type == \"last_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition_type: {condition_type}\")\n",
    "\n",
    "    where_result = np.where(condition)[0]\n",
    "\n",
    "    if len(where_result) > 0:\n",
    "        return vec.time[where_result[index_choice]].values\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_candidate_duration(candidate: pd.Series, data, get_candidate_data_fn:Callable =get_candidate_data_xr) -> pd.Series:\n",
    "    try:\n",
    "        candidate_data = get_candidate_data_fn(candidate, data)\n",
    "        return calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calc_candidate_d_duration(candidate, data , get_candidate_data_fn:Callable =get_candidate_data) -> pd.Series:\n",
    "    try:\n",
    "        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n",
    "            candidate_data = get_candidate_data_fn(candidate, data, neighbor=1)\n",
    "            d_time = candidate['d_time']\n",
    "            threshold = candidate['threshold']\n",
    "            return calc_d_duration(candidate_data, d_time, threshold)\n",
    "        else:\n",
    "            return pandas.Series({\n",
    "                'd_tstart': candidate['d_tstart'],\n",
    "                'd_tstop': candidate['d_tstop'],\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calibrate_candidate_duration(candidate: pd.Series, data:xr.DataArray, data_resolution=data_resolution, ratio = 3/4):\n",
    "    \"\"\"\n",
    "    Calibrates the candidate duration. \n",
    "    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n",
    "    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - pd.Series: The calibrated candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_notnull = pd.notnull(candidate['d_tstart'])\n",
    "    stop_notnull = pd.notnull(candidate['d_tstop']) \n",
    "    \n",
    "    match start_notnull, stop_notnull:\n",
    "        case (True, True):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (True, False):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n",
    "        case (False, True):\n",
    "            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (False, False):\n",
    "            return pandas.Series({\n",
    "                'd_tstart': None,\n",
    "                'd_tstop': None,\n",
    "            })\n",
    "    \n",
    "    duration = d_tstop - d_tstart\n",
    "    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n",
    "    \n",
    "    if num_of_points_between <= (duration/data_resolution) * ratio:\n",
    "        d_tstart = None\n",
    "        d_tstop = None\n",
    "    \n",
    "    return pandas.Series({\n",
    "        'd_tstart': d_tstart,\n",
    "        'd_tstop': d_tstop,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_calc_duration = pdp.PdPipeline([\n",
    "    pdp.ApplyToRows(calc_candidate_duration, func_desc='calculating duration parameters'),\n",
    "    pdp.ApplyToRows(calc_candidate_d_duration, func_desc='calculating duration parameters if needed'),\n",
    "])\n",
    "\n",
    "pdp_calibrate_duration = pdp.PdPipeline([\n",
    "    pdp.ApplyToRows(calibrate_candidate_duration, func_desc='calibrating duration parameters if needed'),\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates = pdp_calc_duration(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID classification\n",
    "\n",
    "In this method, TDs and RDs satisfy $ \\frac{ |B_N| }{ |B_{bg}| } < 0.2$ and $ | \\frac{ \\Delta |B| }{ |B_{bg}| } | > 0.4$ B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with < 0.4 B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as > 0.4 B BN bg ∣∣ ∣∣ , > D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\n",
    "\n",
    "\n",
    "Criteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\n",
    "\n",
    "| Type   |  $\\|B_n/B\\|$      | $\\| \\Delta B / B \\|$  |\n",
    "|----------|-------------|------|\n",
    "| Rotational (RD) | large | small |\n",
    "| Tangential (TD) | small |  large |\n",
    "| Either (ED) | small | small |\n",
    "| Neither (ND) | large | large |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minimum variance analysis (MVA)\n",
    "\n",
    "To ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BnOverB_RD_lower_threshold = 0.4\n",
    "dBOverB_RD_upper_threshold = 0.2\n",
    "\n",
    "BnOverB_TD_upper_threshold = 0.2\n",
    "dBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n",
    "\n",
    "BnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\n",
    "dBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n",
    "\n",
    "BnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\n",
    "dBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True, False]) | np.array([True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspedas.cotrans.minvar import minvar\n",
    "def calc_classification_index(data: xr.DataArray):\n",
    "    \n",
    "    vrot, v, w = minvar(data.to_numpy()) # NOTE: using `.to_numpy()` will significantly speed up the computation.\n",
    "    Vl = v[:,0] # Maximum variance direction eigenvector\n",
    "\n",
    "    B_rot = xr.DataArray(vrot, dims=['time', 'v_dim'], coords={'time': data.time})\n",
    "    B = calc_vec_mag(B_rot)\n",
    "    B_n = B_rot.isel(v_dim=2)\n",
    "    \n",
    "    B_mean = B.mean(dim=\"time\")\n",
    "    B_n_mean = B_n.mean(dim=\"time\")\n",
    "    \n",
    "    BnOverB = B_n_mean / B_mean\n",
    "    # BnOverB = np.abs(B_n / B).mean(dim=\"time\")\n",
    "\n",
    "    dB = B.isel(time=-1) - B.isel(time=0)\n",
    "    dBOverB = np.abs(dB / B_mean)\n",
    "    dBOverB_max = (B.max(dim=\"time\") - B.min(dim=\"time\")) / B_mean\n",
    "    \n",
    "    \n",
    "    return pandas.Series({\n",
    "        'Vl_x': Vl[0],\n",
    "        'Vl_y': Vl[1],\n",
    "        'Vl_z': Vl[2],\n",
    "        'eig0': w[0],\n",
    "        'eig1': w[1],\n",
    "        'eig2': w[2],\n",
    "        'Q_mva': w[1]/w[2],\n",
    "        'B': B_mean.item(),\n",
    "        'B_n': B_n_mean.item(),\n",
    "        'dB': dB.item(),\n",
    "        'BnOverB': BnOverB.item(), \n",
    "        'dBOverB': dBOverB.item(),\n",
    "        'dBOverB_max': dBOverB_max.item(),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_id(BnOverB, dBOverB):\n",
    "    BnOverB = np.abs(np.asarray(BnOverB))\n",
    "    dBOverB = np.asarray(dBOverB)\n",
    "\n",
    "    s1 = (BnOverB > BnOverB_RD_lower_threshold)\n",
    "    s2 = (dBOverB > dBOverB_RD_upper_threshold)\n",
    "    s3 = (BnOverB > BnOverB_TD_upper_threshold)\n",
    "    s4 = s2 # note: s4 = (dBOverB > dBOverB_TD_lower_threshold)\n",
    "    \n",
    "    RD = s1 & ~s2\n",
    "    TD = ~s3 & s4\n",
    "    ED = ~s1 & ~s4\n",
    "    ND = s3 & s2\n",
    "\n",
    "    # Create an empty result array with the same shape\n",
    "    result = np.empty_like(BnOverB, dtype=object)\n",
    "\n",
    "    result[RD] = \"RD\"\n",
    "    result[TD] = \"TD\"\n",
    "    result[ED] = \"ED\"\n",
    "    result[ND] = \"ND\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_classify_id = pdp.PdPipeline([\n",
    "    pdp.ApplyToRows(lambda candidate: calc_classification_index(sat_fgm.sel(time = slice(candidate['d_tstart'], candidate['d_tstop']))), func_desc='calculating index \"q_mva\", \"BnOverB\" and \"dBOverB\"'),\n",
    "    pdp.ColByFrameFunc(\"type\", lambda df: classify_id(df[\"BnOverB\"], df[\"dBOverB\"]),func_desc=\"classifying the type of the ID\")\n",
    "    # pdp.ApplyToRows(lambda candidate: classify_id(candidate[\"BnOverB\"], candidate[\"dBOverB\"]), colname=\"type\", func_desc=\"classifying the type of the ID\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdp_classify_id(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rotation_angle(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector.\n",
    "    - v2: The second vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # convert xr.Dataarray to numpy arrays\n",
    "    if isinstance(v1, DataArray):\n",
    "        v1 = v1.to_numpy()\n",
    "    if isinstance(v2, DataArray):\n",
    "        v2 = v2.to_numpy()\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calc_candidate_rotation_angle(candidates, data:  xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = candidates['d_tstart']\n",
    "    tstop = candidates['d_tstop']\n",
    "    \n",
    "    # Convert Series to numpy arrays if necessary\n",
    "    if isinstance(tstart, pd.Series):\n",
    "        tstart = tstart.to_numpy()\n",
    "        tstop = tstop.to_numpy()\n",
    "        # no need to Handle NaT values (as `calibrate_candidate_duration` will handle this)\n",
    "    \n",
    "    # Get the vectors at the two time steps\n",
    "    vecs_before = data.sel(time=tstart, method=\"nearest\")\n",
    "    vecs_after = data.sel(time=tstop, method=\"nearest\")\n",
    "    \n",
    "    # Compute the rotation angle(s)\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_calc_rotation_angle = pdp.ColByFrameFunc(\"rotation_angle\", lambda df: calc_candidate_rotation_angle(df, data=sat_fgm), func_desc='calculating rotation angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign satellite locations to the discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_location(candidate, location_data: DataArray):\n",
    "    return location_data.sel(time = candidate['d_time']).to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_assign_coordinates = pdp.PdPipeline([\n",
    "    pdp.ApplyToRows(lambda candidate: get_candidate_location(candidate, sat_state), func_desc='assigning coordinates'),\n",
    "    # TODO: can we use `pdp.ColByFrameFunc` here?\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ID_filter_condition(\n",
    "    index_std_threshold = 2,\n",
    "    index_fluc_threshold = 1,\n",
    "    index_diff_threshold = 0.1,\n",
    "    sparse_num = 15\n",
    "):\n",
    "    return (\n",
    "        (pl.col(\"index_std\") > index_std_threshold)\n",
    "        & (pl.col(\"index_fluctuation\") > index_fluc_threshold)\n",
    "        & (pl.col(\"index_diff\") > index_diff_threshold)\n",
    "        & (\n",
    "            pl.col(\"index_std\").is_finite()\n",
    "        )  # for cases where neighboring groups have std=0\n",
    "        & (\n",
    "            pl.col(\"count\") > sparse_num\n",
    "        )  # filter out sparse intervals, which may give unreasonable results.\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpipe.util import out_of_place_col_insert\n",
    "\n",
    "class ApplyToRows(pdp.ApplyToRows):\n",
    "    \"\"\"A pipeline stage that works with `modin` DataFrames.\n",
    "    \"\"\"\n",
    "    def _transform(self, X, verbose):\n",
    "        new_cols = X.apply(self._func, axis=1)\n",
    "        if isinstance(new_cols, pd.Series):\n",
    "            loc = len(X.columns)\n",
    "            if self._follow_column:\n",
    "                loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            return out_of_place_col_insert(\n",
    "                X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "            )\n",
    "        if isinstance(new_cols, pd.DataFrame):\n",
    "            sorted_cols = sorted(list(new_cols.columns))\n",
    "            new_cols = new_cols[sorted_cols]\n",
    "            if self._follow_column:\n",
    "                inter_X = X\n",
    "                loc = X.columns.get_loc(self._follow_column) + 1\n",
    "                for colname in new_cols.columns:\n",
    "                    inter_X = out_of_place_col_insert(\n",
    "                        X=inter_X,\n",
    "                        series=new_cols[colname],\n",
    "                        loc=loc,\n",
    "                        column_name=colname,\n",
    "                    )\n",
    "                    loc += 1\n",
    "                return inter_X\n",
    "            assign_map = {\n",
    "                colname: new_cols[colname] for colname in new_cols.columns\n",
    "            }\n",
    "            return X.assign(**assign_map)\n",
    "        raise TypeError(  # pragma: no cover\n",
    "            \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "            \" Only Series and DataFrame are allowed.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_candidate_classification_index(candidate, data):\n",
    "    return calc_classification_index(\n",
    "        data.sel(time=slice(candidate[\"d_tstart\"], candidate[\"d_tstop\"]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDsPipeline:\n",
    "    def __init__(self, sat_fgm=None, sat_state=None):\n",
    "        self.sat_fgm = sat_fgm\n",
    "        self.sat_state = sat_state\n",
    "\n",
    "        self.pipelines = {}\n",
    "\n",
    "    # fmt: off\n",
    "    def add_calc_duration(self):\n",
    "        self.pipelines[\"calc_duration\"] = pdp.PdPipeline([\n",
    "            ApplyToRows(\n",
    "                lambda candidate: calc_candidate_duration(candidate, self.sat_fgm),\n",
    "                func_desc=\"calculating duration parameters\"\n",
    "            ),\n",
    "            ApplyToRows(\n",
    "                lambda candidate: calc_candidate_d_duration(candidate, self.sat_fgm),\n",
    "                func_desc=\"calculating duration parameters if needed\"\n",
    "            ),\n",
    "        ])\n",
    "        return self\n",
    "\n",
    "    def add_calibrate_duration(self):\n",
    "        self.pipelines[\"calibrate_duration\"] = \\\n",
    "            ApplyToRows(\n",
    "                lambda candidate: calibrate_candidate_duration(candidate, self.sat_fgm),\n",
    "                func_desc=\"calibrating duration parameters if needed\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def add_classify_id(self):\n",
    "        self.pipelines[\"classify_id\"] = pdp.PdPipeline([\n",
    "            ApplyToRows(\n",
    "                lambda candidate: calc_candidate_classification_index(candidate, self.sat_fgm),\n",
    "                func_desc='calculating index \"q_mva\", \"BnOverB\" and \"dBOverB\"'\n",
    "            ),\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"type\",\n",
    "                lambda df: classify_id(df[\"BnOverB\"], df[\"dBOverB\"]),\n",
    "                func_desc=\"classifying the type of the ID\"\n",
    "            ),\n",
    "        ])\n",
    "        return self\n",
    "    \n",
    "    def add_calc_rotation_angle(self):\n",
    "        self.pipelines[\"calc_rotation_angle\"] = \\\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"rotation_angle\",\n",
    "                lambda df: calc_candidate_rotation_angle(df, data=self.sat_fgm),\n",
    "                func_desc=\"calculating rotation angle\"\n",
    "            )\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def add_assign_coordinates(self):\n",
    "        self.pipelines[\"assign_coordinates\"] = \\\n",
    "            ApplyToRows(\n",
    "                lambda candidate: get_candidate_location(candidate, self.sat_state),\n",
    "                func_desc=\"assigning coordinates\"\n",
    "            )\n",
    "        # Return a new instance with added stage\n",
    "        return self\n",
    "\n",
    "    # fmt: on\n",
    "\n",
    "    # ... you can add more methods as needed\n",
    "\n",
    "    def get_pipeline(self, name):\n",
    "        return self.pipelines.get(name)\n",
    "    \n",
    "# def process_candidates(\n",
    "#     candidates: pl.DataFrame, sat_fgm: xr.DataArray, sat_state: xr.DataArray\n",
    "# ):\n",
    "#     candidates = convert_to_dataframe(candidates)\n",
    "#     builder = IDsPipeline(sat_fgm, sat_state)\n",
    "\n",
    "#     # Build pipelines\n",
    "#     id_pipelines = builder.add_calc_duration().add_calibrate_duration().add_classify_id().add_calc_rotation_angle().add_assign_coordinates()\n",
    "\n",
    "#     candidates = id_pipelines.get_pipeline(\"calc_duration\").apply(candidates)\n",
    "\n",
    "#     # calibrate duration\n",
    "#     temp_candidates = candidates.loc[\n",
    "#         lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "#     ]\n",
    "#     if not temp_candidates.empty:\n",
    "#         candidates.update(\n",
    "#             id_pipelines.get_pipeline(\"calibrate_duration\").apply(temp_candidates)\n",
    "#         )\n",
    "\n",
    "#     candidates = pdp.DropNa()(candidates)  # Remove candidates with NaN values\n",
    "\n",
    "#     # Apply remaining pipelines (you can refactor this further if needed)\n",
    "#     ids = (\n",
    "#         id_pipelines.get_pipeline(\"classify_id\") + \n",
    "#         id_pipelines.get_pipeline(\"calc_rotation_angle\") +\n",
    "#         id_pipelines.get_pipeline(\"assign_coordinates\")\n",
    "#     ).apply(candidates)\n",
    "#     # Add other pipelines as needed\n",
    "\n",
    "#     return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JUNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[32m2023-09-21 02:46:32.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2011 as the output file already exists.\u001b[0m\n",
      "\u001b[32m2023-09-21 02:46:32.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2012 as the output file already exists.\u001b[0m\n",
      "\u001b[32m2023-09-21 02:46:32.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2013 as the output file already exists.\u001b[0m\n",
      "\u001b[32m2023-09-21 02:46:32.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2014 as the output file already exists.\u001b[0m\n",
      "\u001b[32m2023-09-21 02:46:32.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2015 as the output file already exists.\u001b[0m\n",
      "\u001b[32m2023-09-21 02:46:32.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 9>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSkipping 2016 as the output file already exists.\u001b[0m\n",
      "100%|██████████| 6/6 [00:00<00:00, 1111.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "# if True:\n",
    "    # year = 2011\n",
    "for year in tqdm(range(starting_year, ending_year+1)):\n",
    "    files = f'data/{sat}_{year}.parquet'\n",
    "    output = f'data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "    \n",
    "    if os.path.exists(output):\n",
    "        logger.info(f\"Skipping {year} as the output file already exists.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "    sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "    sat_state = df2ts(data, [\"X\", \"Y\", \"Z\"], attrs={\"coordinate_system\": coord, \"units\": \"km\"})\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "    \n",
    "    ids = process_candidates(candidates, sat_fgm, sat_state)\n",
    "    df = pandas.DataFrame(ids)\n",
    "    df.to_parquet(output)\n",
    "    # pandas.DataFrame(ids).to_parquet(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Sep-23 02:48:20: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n"
     ]
    }
   ],
   "source": [
    "# Test different libraries to parallelize the computation\n",
    "test = True\n",
    "if test:\n",
    "    pdp_test = ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, jno_fgm)),\n",
    "        # lambda candidate: calc_duration(jno_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    candidates_pd = candidates.to_pandas()\n",
    "    candidates_modin = pd.DataFrame(candidates_pd)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # Standard case: 37+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 60 secs\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works, 8 secs # not work? `DataFrame' object has no attribute 'mapply'\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works with ray, 8 secs # NOTE: can not work with dask\n",
    "    # pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=jno_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n",
    "    # pdp_test(candidates_modin) # TypeError: Unexpected type generated by applying a function to a DataFrame. Only Series and DataFrame are allowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THEMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]21-Sep-23 03:05:26: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "  0%|          | 0/6 [01:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zijin/projects/planet/main.ipynb Cell 72\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m filter_condition \u001b[39m=\u001b[39m get_ID_filter_condition(sparse_num \u001b[39m=\u001b[39m sparse_num)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m candidates \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mfilter(filter_condition)\u001b[39m.\u001b[39mwith_columns(pl_format_time(tau))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m ids \u001b[39m=\u001b[39m process_candidates(candidates, sat_fgm, sat_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df \u001b[39m=\u001b[39m pandas\u001b[39m.\u001b[39mDataFrame(ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df\u001b[39m.\u001b[39mto_parquet(output)\n",
      "\u001b[1;32m/Users/zijin/projects/planet/main.ipynb Cell 72\u001b[0m line \u001b[0;36mprocess_candidates\u001b[0;34m(candidates, sat_fgm, sat_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m pdp_assign_coordinates \u001b[39m=\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     ApplyToRows(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m candidate: get_candidate_location(candidate, sat_state),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         func_desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39massigning coordinates\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# TODO: can we use `pdp.ColByFrameFunc` here?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# fmt: on\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m candidates \u001b[39m=\u001b[39m pdp_calc_duration\u001b[39m.\u001b[39;49mapply(candidates)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# calibrate duration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m temp_candidates \u001b[39m=\u001b[39m candidates\u001b[39m.\u001b[39mloc[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m df: df[\u001b[39m\"\u001b[39m\u001b[39md_tstart\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misnull() \u001b[39m|\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39md_tstop\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misnull()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/pdpipe/core.py:1282\u001b[0m, in \u001b[0;36mPdPipeline.apply\u001b[0;34m(self, X, y, exraise, verbose, time, fit_context, application_context)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\n\u001b[1;32m   1274\u001b[0m         X,\n\u001b[1;32m   1275\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         application_context\u001b[39m=\u001b[39mapplication_context,\n\u001b[1;32m   1280\u001b[0m     )\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 1282\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(\n\u001b[1;32m   1283\u001b[0m     X,\n\u001b[1;32m   1284\u001b[0m     y,\n\u001b[1;32m   1285\u001b[0m     exraise\u001b[39m=\u001b[39;49mexraise,\n\u001b[1;32m   1286\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1287\u001b[0m     time\u001b[39m=\u001b[39;49mtime,\n\u001b[1;32m   1288\u001b[0m     fit_context\u001b[39m=\u001b[39;49mfit_context,\n\u001b[1;32m   1289\u001b[0m     application_context\u001b[39m=\u001b[39;49mapplication_context,\n\u001b[1;32m   1290\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/pdpipe/core.py:1424\u001b[0m, in \u001b[0;36mPdPipeline.fit_transform\u001b[0;34m(self, X, y, exraise, verbose, time, fit_context, application_context)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     stage\u001b[39m.\u001b[39mapplication_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplication_context\n\u001b[1;32m   1422\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_dynamics(stage, inter_X, inter_y)\n\u001b[0;32m-> 1424\u001b[0m     inter_X \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit_transform(\n\u001b[1;32m   1425\u001b[0m         X\u001b[39m=\u001b[39;49minter_X,\n\u001b[1;32m   1426\u001b[0m         y\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1427\u001b[0m         exraise\u001b[39m=\u001b[39;49mexraise,\n\u001b[1;32m   1428\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[1;32m   1430\u001b[0m     stage\u001b[39m.\u001b[39mapplication_context \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/pdpipe/core.py:701\u001b[0m, in \u001b[0;36mPdPipelineStage.fit_transform\u001b[0;34m(self, X, y, exraise, verbose)\u001b[0m\n\u001b[1;32m    698\u001b[0m     res_X, res_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_Xy(\n\u001b[1;32m    699\u001b[0m         X, y, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m    700\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m     res_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    702\u001b[0m     res_y \u001b[39m=\u001b[39m y\n\u001b[1;32m    703\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fitted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/pdpipe/core.py:489\u001b[0m, in \u001b[0;36mPdPipelineStage._fit_transform\u001b[0;34m(self, X, verbose)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_transform\u001b[39m(\n\u001b[1;32m    471\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    472\u001b[0m     X: pandas\u001b[39m.\u001b[39mDataFrame,\n\u001b[1;32m    473\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    474\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pandas\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m    475\u001b[0m     \u001b[39m\"\"\"Fits this stage and transforms the input dataframe.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \n\u001b[1;32m    477\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39m        The transformed dataframe.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, verbose\u001b[39m=\u001b[39;49mverbose)\n",
      "\u001b[1;32m/Users/zijin/projects/planet/main.ipynb Cell 72\u001b[0m line \u001b[0;36mApplyToRows._transform\u001b[0;34m(self, X, verbose)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, X, verbose):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     new_cols \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(new_cols, pd\u001b[39m.\u001b[39mSeries):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zijin/projects/planet/main.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         loc \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m obj(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/pandas/dataframe.py:419\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39m# the 'else' branch also handles 'result_type == \"expand\"' since it makes the output type\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39m# depend on the `func` result (Series for a scalar, DataFrame for list-like)\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     reduced_index \u001b[39m=\u001b[39m pandas\u001b[39m.\u001b[39mIndex([MODIN_UNNAMED_SERIES_LABEL])\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mif\u001b[39;00m query_compiler\u001b[39m.\u001b[39;49mget_axis(axis)\u001b[39m.\u001b[39mequals(\n\u001b[1;32m    420\u001b[0m         reduced_index\n\u001b[1;32m    421\u001b[0m     ) \u001b[39mor\u001b[39;00m query_compiler\u001b[39m.\u001b[39mget_axis(axis \u001b[39m^\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mequals(reduced_index):\n\u001b[1;32m    422\u001b[0m         output_type \u001b[39m=\u001b[39m Series\n\u001b[1;32m    423\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m obj(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/storage_formats/base/query_compiler.py:4080\u001b[0m, in \u001b[0;36mBaseQueryCompiler.get_axis\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m   4066\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_axis\u001b[39m(\u001b[39mself\u001b[39m, axis):\n\u001b[1;32m   4067\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4068\u001b[0m \u001b[39m    Return index labels of the specified axis.\u001b[39;00m\n\u001b[1;32m   4069\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4078\u001b[0m \u001b[39m    pandas.Index\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/storage_formats/pandas/query_compiler.py:90\u001b[0m, in \u001b[0;36m_get_axis.<locals>.<lambda>\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modin_frame\u001b[39m.\u001b[39mindex\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modin_frame\u001b[39m.\u001b[39;49mcolumns\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/dataframe/pandas/dataframe/dataframe.py:544\u001b[0m, in \u001b[0;36mPandasDataframe._get_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m     columns, column_widths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_columns_cache\u001b[39m.\u001b[39mget(return_lengths\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     columns, column_widths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_axis_labels_and_lengths(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_columns_cache(columns)\n\u001b[1;32m    546\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_column_widths_cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m obj(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/dataframe/pandas/dataframe/dataframe.py:630\u001b[0m, in \u001b[0;36mPandasDataframe._compute_axis_labels_and_lengths\u001b[0;34m(self, axis, partitions)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m partitions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     partitions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitions\n\u001b[0;32m--> 630\u001b[0m new_index, internal_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partition_mgr_cls\u001b[39m.\u001b[39;49mget_indices(axis, partitions)\n\u001b[1;32m    631\u001b[0m \u001b[39mreturn\u001b[39;00m new_index, \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlen\u001b[39m, internal_idx))\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m obj(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/dataframe/pandas/partitioning/partition_manager.py:933\u001b[0m, in \u001b[0;36mPandasDataframePartitionManager.get_indices\u001b[0;34m(cls, axis, partitions, index_func)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(target):\n\u001b[1;32m    932\u001b[0m     new_idx \u001b[39m=\u001b[39m [idx\u001b[39m.\u001b[39mapply(func) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m target[\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 933\u001b[0m     new_idx \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_objects_from_partitions(new_idx)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     new_idx \u001b[39m=\u001b[39m [pandas\u001b[39m.\u001b[39mIndex([])]\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m obj(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/dataframe/pandas/partitioning/partition_manager.py:874\u001b[0m, in \u001b[0;36mPandasDataframePartitionManager.get_objects_from_partitions\u001b[0;34m(cls, partitions)\u001b[0m\n\u001b[1;32m    870\u001b[0m             partitions[idx] \u001b[39m=\u001b[39m part\u001b[39m.\u001b[39mforce_materialization()\n\u001b[1;32m    871\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    872\u001b[0m         [\u001b[39mlen\u001b[39m(partition\u001b[39m.\u001b[39mlist_of_blocks) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m partition \u001b[39min\u001b[39;00m partitions]\n\u001b[1;32m    873\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mImplementation assumes that each partition contains a single block.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 874\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_execution_wrapper\u001b[39m.\u001b[39;49mmaterialize(\n\u001b[1;32m    875\u001b[0m         [partition\u001b[39m.\u001b[39;49mlist_of_blocks[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m partition \u001b[39min\u001b[39;49;00m partitions]\n\u001b[1;32m    876\u001b[0m     )\n\u001b[1;32m    877\u001b[0m \u001b[39mreturn\u001b[39;00m [partition\u001b[39m.\u001b[39mget() \u001b[39mfor\u001b[39;00m partition \u001b[39min\u001b[39;00m partitions]\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/modin/core/execution/ray/common/engine_wrapper.py:92\u001b[0m, in \u001b[0;36mRayWrapper.materialize\u001b[0;34m(cls, obj_id)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmaterialize\u001b[39m(\u001b[39mcls\u001b[39m, obj_id):\n\u001b[1;32m     79\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m    Get the value of object from the Plasma store.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m        Whatever was identified by `obj_id`.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m ray\u001b[39m.\u001b[39;49mget(obj_id)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_init_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/ray/_private/worker.py:2541\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject_refs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must either be an ObjectRef or a list of ObjectRefs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2538\u001b[0m     )\n\u001b[1;32m   2540\u001b[0m \u001b[39m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2541\u001b[0m values, debugger_breakpoint \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mget_objects(object_refs, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   2542\u001b[0m \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m   2543\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/mambaforge/envs/cool_planet/lib/python3.10/site-packages/ray/_private/worker.py:756\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to call `get` on the value \u001b[39m\u001b[39m{\u001b[39;00mobject_ref\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhich is not an ray.ObjectRef.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m         )\n\u001b[1;32m    755\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m) \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 756\u001b[0m data_metadata_pairs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39;49mget_objects(\n\u001b[1;32m    757\u001b[0m     object_refs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_task_id, timeout_ms\n\u001b[1;32m    758\u001b[0m )\n\u001b[1;32m    759\u001b[0m debugger_breakpoint \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    760\u001b[0m \u001b[39mfor\u001b[39;00m data, metadata \u001b[39min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3110\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:445\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sat = 'thb'\n",
    "coord = 'gse'\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=4)\n",
    "\n",
    "files = f'data/{sat}_fgs_{coord}.parquet'\n",
    "output = f'data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "if os.path.exists(output):\n",
    "    logger.info(f\"Skipping {year} as the output file already exists.\")\n",
    "    continue\n",
    "\n",
    "data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "sat_state = pandas.read_parquet(f'data/{sat}_state.parquet')\n",
    "\n",
    "indices = compute_indices(data, tau)\n",
    "# filter condition\n",
    "sparse_num = tau / data_resolution // 3\n",
    "filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "\n",
    "ids = process_candidates(candidates, sat_fgm, sat_state)\n",
    "df = ids.to_pandas()\n",
    "df.to_parquet(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdp_calc_duration)\n",
    "if test:\n",
    "    candidates = pdp_calc_duration.apply(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Inspect interesting candidates\n",
    "if False:\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]\n",
    "    num = 28\n",
    "    # temp_candidate = candidates.iloc[num]\n",
    "    temp_candidate = temp_candidates.iloc[num]\n",
    "    print(temp_candidate)\n",
    "    plot_candidate(temp_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdp_calibrate_duration)\n",
    "\n",
    "if test: \n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]\n",
    "    if not temp_candidates.empty:\n",
    "        display(temp_candidates)\n",
    "        candidates.update(\n",
    "            pdp_calibrate_duration.apply(temp_candidates)\n",
    "        )  # This step is needed to classify the candidates\n",
    "\n",
    "    candidates = pdp.DropNa()(candidates) # drop candidates with NaN values\n",
    "    # pdp.RowDrop({\"d_tstart\": lambda x: pd.isnull(x), \"d_tstop\": lambda x: pd.isnull(x)}, reduce=\"all\",)(candidates)\n",
    "    # pdp.RowDrop([lambda x: pd.isnull(x)], reduce='all', columns=['d_tstart', 'd_tstop'])(candidates) # Notes: slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdp_classify_id)\n",
    "if test: \n",
    "    candidates = pdp_classify_id(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdp_calc_rotation_angle)\n",
    "if test: \n",
    "    candidates = pdp_calc_rotation_angle(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdp_assign_coordinates)\n",
    "if test: \n",
    "    candidates = pdp_assign_coordinates(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = pdp_calc_duration + pdp_calibrate_duration+ pdp_classify_id +  pdp_calc_rotation_angle + pdp_assign_coordinates\n",
    "print(pipelines)\n",
    "# candidates = pipelines(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read candidates from files in current directory\n",
    "pattern = 'data/candidates*.parquet'\n",
    "data = Path() / pattern\n",
    "\n",
    "candidates = pl.scan_parquet(data).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(candidates.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting candidates of different types of discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates, candidate_type='TD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates, candidate_type='RD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates, candidate_type='ED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates, candidate_type='ND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurrence rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the occurence rates of different types of ID\n",
    "def occurence_rate(candidates, candidate_type):\n",
    "    return len(candidates[candidates['type'] == candidate_type]) / len(candidates)\n",
    "\n",
    "def time_occurence_rate(candidates):\n",
    "    if len(candidates) <= 1:\n",
    "        return None\n",
    "    else:\n",
    "        return (candidates.iloc[-1]['tstop'] - candidates.iloc[0]['tstart']) / (len(candidates) -1)\n",
    "\n",
    "CANDIDATE_TYPES = ['RD', 'TD', 'ED', 'ND']\n",
    "\n",
    "for candidate_type in CANDIDATE_TYPES:\n",
    "    logger.info(f\"Occurrence rate of {candidate_type}: {occurence_rate(candidates, candidate_type)}\")\n",
    "    logger.info(f\"Time occurrence rate of {candidate_type}: {time_occurence_rate(candidates[candidates['type'] == candidate_type])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp.ColByFrameFunc(\"R\", lambda df: df[['X','Y', 'Z']].apply(np.linalg.norm, axis=1), func_desc='calculating R')(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.plot(x=\"X\", y=\"d_star\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates.update(pdp_calibrate_duration.apply(temp_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_candidates = get_candidates(candidates, 'RD')\n",
    "temp_candidates = pdp_calc_duration(temp_candidates)\n",
    "temp_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(temp_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test minvar and principal axes vectors\n",
    "test_data = np.array([[1,1,0],[-1,-1,0]])\n",
    "vrot, v, w = minvar(test_data)\n",
    "Vi = v[:,0]\n",
    "print(Vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test minvar_matrix_make\n",
    "in_var_name = \"fgm\"\n",
    "\n",
    "vrot, v, w = minvar(get_data(in_var_name, xarray=True))\n",
    "\n",
    "minvar_matrix_make(in_var_name)\n",
    "tvector_rotate(f'{in_var_name}_mva_mat', in_var_name)\n",
    "(get_data(f\"{in_var_name}_rot\").y==vrot).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_candidates(candidates.loc[lambda _: _['time']=='2012-07-10 02:31:15'])\n",
    "temp_trange = ['2012-07-15 03:44', '2012-07-15 03:47']\n",
    "temp_data = sat_fgm.sel(time=slice(*temp_trange))\n",
    "temp_data.plot.scatter(x='time', hue='v_dim')\n",
    "# temp_data.resample(time=pd.Timedelta(tau, unit='s')).map(calc_vec_std)\n",
    "compute_index_std(temp_data, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: neighboring data is missing, causing the calculation of the standard deviation index to be Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case: neighboring data is missing, causing the calculation of the standard deviation index to be Inf.\n",
    "temp_trange = ['2012-07-10 02:30', '2012-07-10 02:32']\n",
    "temp_data = sat_fgm.sel(time=slice(*temp_trange))\n",
    "temp_data.plot.scatter(x='time', hue='v_dim')\n",
    "# temp_data.resample(time=pd.Timedelta(tau, unit='s')).map(calc_vec_std)\n",
    "compute_index_std(temp_data, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Not very accurate for waving magnetic field..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_candidate = {'time': Timestamp('2012-05-01 00:39:12'),\n",
    " 'tstart': Timestamp('2012-05-01 00:38:56'),\n",
    " 'tstop': Timestamp('2012-05-01 00:39:28'),\n",
    " 'i1': 2.891042053414383,\n",
    " 'i2': 2.389699609352786,\n",
    " 'i3': 1.3916002784658887,\n",
    " 'd_star': 0.27143595,\n",
    " 'd_time': Timestamp('2012-05-01 00:39:18.672000'),\n",
    " 'd_tstart': Timestamp('2012-05-01 00:39:14.672000'),\n",
    " 'd_tstop': Timestamp('2012-05-01 00:39:19.671000'),\n",
    "}\n",
    "\n",
    "plot_candidate(temp_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_candidate_data_xr(temp_candidate, neighbor=1)\n",
    "vec_diff = data.differentiate(\"time\", datetime_unit=\"s\", edge_order=2).isel(time=slice(1,-1))\n",
    "vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "vec_diff_mag.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Small threshold_ratio values will tend to make the duration longer if the duration can be determined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different threshold ratios\n",
    "threshold_ratios = [1/8, 1/4, 0.3, 1/3, 1/2]\n",
    "for threshold_ratio in threshold_ratios:\n",
    "    temp_candidate.update(calc_duration(get_candidate_data_xr(temp_candidate), threshold_ratio=threshold_ratio).to_dict())\n",
    "    plot_candidate(temp_candidate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_solar_wind",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
