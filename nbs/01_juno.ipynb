{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from Juno\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "output-file: juno.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: hide\n",
    "#| code-summary: import all the packages needed for the project\n",
    "\n",
    "from ids_finder.utils import *\n",
    "from ids_finder.core import *\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "import polars as pl\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "\n",
    "import pandas\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import pdr\n",
    "\n",
    "\n",
    "import pytplot\n",
    "from pytplot import timebar\n",
    "from pytplot import get_data, store_data, tplot, split_vec, join_vec, tplot_options, options, tlimit, highlight, degap\n",
    "\n",
    "import pdpipe as pdp\n",
    "\n",
    "from typing import Callable\n",
    "from pandas import (\n",
    "    DataFrame,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacecraft-Solar equatorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate System of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **SE (Solar Equatorial)**\n",
    "    - Code: `se`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `se_rN[N]s`\n",
    "        - Resampled 1 hour: `se_r1h`\n",
    "\n",
    "2. **PC (Planetocentric)**\n",
    "    - Code: `pc`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `pc_rN[N]s`\n",
    "        \n",
    "3. **SS (Sun-State)**\n",
    "    - Code: `ss`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `ss_rN[N]s`\n",
    "        \n",
    "4. **PL (Payload)**\n",
    "    - Code: `pl`\n",
    "    - Resampling options: \n",
    "        - Number of seconds (1 or 60): `pl_rN[N]s`\n",
    "\n",
    "\n",
    "```txt\n",
    "------------------------------------------------------------------------------\n",
    "Juno Mission Phases                                                           \n",
    "------------------------------------------------------------------------------\n",
    "Start       Mission                                                           \n",
    "Date        Phase                                                             \n",
    "==============================================================================\n",
    "2011-08-05  Launch                                                            \n",
    "2011-08-08  Inner Cruise 1                                                    \n",
    "2011-10-10  Inner Cruise 2                                                    \n",
    "2013-05-28  Inner Cruise 3                                                    \n",
    "2013-11-05  Quiet Cruise                                                      \n",
    "2016-01-05  Jupiter Approach                                                  \n",
    "2016-06-30  Jupiter Orbital Insertion                                         \n",
    "2016-07-05  Capture Orbit                                                     \n",
    "2016-10-19  Period Reduction Maneuver                                         \n",
    "2016-10-20  Orbits 1-2                                                        \n",
    "2016-11-09  Science Orbits                                                    \n",
    "2017-10-11  Deorbit\n",
    "```\n",
    "\n",
    "```txt\n",
    "File Naming Convention                                                        \n",
    "==============================================================================\n",
    "Convention:                                                                   \n",
    "   fgm_jno_LL_CCYYDDDxx_vVV.ext                                               \n",
    "Where:                                                                        \n",
    "   fgm - Fluxgate Magnetometer three character instrument abbreviation        \n",
    "   jno - Juno                                                                 \n",
    "    LL - CODMAC Data level, for example, l3 for level 3                       \n",
    "    CC - The century portion of a date, 20                                    \n",
    "    YY - The year of century portion of a date, 00-99                         \n",
    "   DDD - The day of year, 001-366                                             \n",
    "    xx - Coordinate system of data (se = Solar equatorial, ser = Solar        \n",
    "         equatorial resampled, pc = Planetocentric, ss = Sun-State,           \n",
    "         pl = Payload)                                                        \n",
    "     v - separator to denote Version number                                   \n",
    "    VV - version                                                              \n",
    "   ext - file extension (sts = Standard Time Series (ASCII) file, lbl = Label \n",
    "         file)                                                                \n",
    "Example:                                                                      \n",
    "   fgm_jno_l3_2014055se_v00.sts    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_dir = \"https://pds-ppi.igpp.ucla.edu/data\"\n",
    "\n",
    "possible_coords = [\"se\", \"ser\", \"pc\", \"ss\", \"pl\"]\n",
    "possible_exts = [\"sts\", \"lbl\"]\n",
    "possible_data_rates = [\"1s\", \"1min\", \"1h\"]\n",
    "\n",
    "juno_ss_config = {\n",
    "    \"DATA_SET_ID\": \"JNO-SS-3-FGM-CAL-V1.0\",\n",
    "    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n",
    "}\n",
    "\n",
    "juno_j_config = {\n",
    "    \"DATA_SET_ID\": \"JNO-J-3-FGM-CAL-V1.0\",\n",
    "    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def download_and_read_lbl_file(config, index_table=False):\n",
    "    \"\"\"Download and read file for each config.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The data read from the file.\n",
    "    \"\"\"\n",
    "    # BUG: index file is not formatted properly according to `lbl` file, so can not be used with `pdr` for.\n",
    "    # ValueError: time data \"282T00:00:31.130,2019\" doesn't match format \"%Y-%jT%H:%M:%S.%f\", at position 3553. You might want to try:\n",
    "    # - passing `format` if your strings have a consistent format;\n",
    "    # - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
    "    # - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
    "\n",
    "    local_dir = os.path.join(os.environ[\"HOME\"], \"juno\", config[\"DATA_SET_ID\"])\n",
    "    base_url = f\"{pds_dir}/{config['DATA_SET_ID']}\"\n",
    "\n",
    "    lbl_fn = config[\"FILE_SPECIFICATION_NAME\"]\n",
    "\n",
    "    if not index_table:\n",
    "        parquet_fn = lbl_fn.replace(\"lbl\", \"parquet\")\n",
    "        parquet_fp = os.path.join(local_dir, parquet_fn)\n",
    "        if os.path.exists(parquet_fp):\n",
    "            return pandas.read_parquet(os.path.join(local_dir, parquet_fn))\n",
    "\n",
    "    lbl_file_url = f\"{base_url}/{lbl_fn}\"\n",
    "    lbl_fp = download_file(lbl_file_url, local_dir, lbl_fn)\n",
    "    logger.debug(f\"Reading {lbl_fp}\")\n",
    "\n",
    "    if index_table:\n",
    "        tab_fn = lbl_fn.replace(\"LBL\", \"TAB\")\n",
    "        tab_fp = download_file(f\"{base_url}/{tab_fn}\", local_dir, tab_fn)\n",
    "        tab_index = pandas.read_csv(tab_fp, delimiter=\",\", quotechar='\"')\n",
    "        tab_index.columns = tab_index.columns.str.replace(\" \", \"\")\n",
    "        return tab_index\n",
    "    else:\n",
    "        sts_fn = lbl_fn.replace(\"lbl\", \"sts\")\n",
    "        download_file(f\"{base_url}/{sts_fn}\", local_dir, sts_fn)\n",
    "        return pdr.read(lbl_fp).TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "jno_ss_index = download_and_read_lbl_file(juno_ss_config, index_table=True)\n",
    "jno_j_index = download_and_read_lbl_file(juno_j_config, index_table=True)\n",
    "\n",
    "_index_time_format = \"%Y-%jT%H:%M:%S.%f\"\n",
    "\n",
    "jno_pipeline = pdp.PdPipeline(\n",
    "    [\n",
    "        pdp.ColDrop([\"PRODUCT_ID\", \"CR_DATE\", \"PRODUCT_LABEL_MD5CHECKSUM\"]),\n",
    "        pdp.ApplyByCols(\"SID\", str.rstrip),\n",
    "        pdp.ApplyByCols(\"FILE_SPECIFICATION_NAME\", str.rstrip),\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"START_TIME\",\n",
    "            lambda df: pandas.to_datetime(df[\"START_TIME\"], format=_index_time_format),\n",
    "        ),\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"STOP_TIME\",\n",
    "            lambda df: pandas.to_datetime(df[\"STOP_TIME\"], format=_index_time_format),\n",
    "        ),\n",
    "        # pdp.ApplyByCols(['START_TIME', 'STOP_TIME'], pandas.to_datetime, format=_index_time_format), # NOTE: This is slow\n",
    "    ]\n",
    ")\n",
    "\n",
    "jno_ss_index = jno_pipeline(jno_ss_index)\n",
    "jno_j_index = jno_pipeline(jno_j_index)\n",
    "\n",
    "index_df = pandas.concat(\n",
    "    [jno_ss_index, jno_j_index], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "starting_date = jno_ss_index['START_TIME'].min().date()\n",
    "ending_date = jno_ss_index['STOP_TIME'].max().date()\n",
    "\n",
    "print(f\"Starting date: {starting_date}\")\n",
    "print(f\"Ending date: {ending_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "available_dates = pandas.concat([jno_ss_index['START_TIME'].dt.date, jno_ss_index['STOP_TIME'].dt.date]).unique()\n",
    "full_year_range = pandas.date_range(start=starting_date, end=ending_date)\n",
    "\n",
    "missing_dates = full_year_range[~full_year_range.isin(available_dates)]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(f\"No days are missing.\")\n",
    "else:\n",
    "    print(f\"The following days are missing\")\n",
    "    print(coll_repr(missing_dates.map(lambda x: x.strftime(\"%Y-%m-%d\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget -r --no-parent --no-clobber 'https://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC/'\n",
    "# aria2c -x 16 -s 16 'https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE/1SEC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 day of data resampled by 1 sec is about 12 MB.\n",
    "\n",
    "So 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\n",
    "\n",
    "Downloading rate is about 250 KB/s, so it will take about 3 days to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_files = 6*365\n",
    "jno_file_size = 12e3\n",
    "thm_file_size = 40e3\n",
    "files_size = jno_file_size + thm_file_size\n",
    "downloading_rate = 250\n",
    "processing_rate = 1/60\n",
    "\n",
    "time_to_download = num_of_files * files_size / downloading_rate / 60 / 60\n",
    "space_required = num_of_files * files_size / 1e6\n",
    "time_to_process = num_of_files / processing_rate / 60 / 60\n",
    "\n",
    "print(f\"Time to download: {time_to_download:.2f} hours\")\n",
    "print(f\"Disk space required: {space_required:.2f} GB\")\n",
    "print(f\"Time to process: {time_to_process:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all files from `lbl` to `parquet` for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Convert data from `lbl` to `parquet` format\n",
    "def lbl2parquet(src: Path, dest: Path) -> None:\n",
    "    df = pdr.read(src).TABLE\n",
    "    df.to_parquet(dest)\n",
    "\n",
    "\n",
    "def convert_file(\n",
    "    file_path: Path, target_format: str, conversion_func: Callable, check_exist=True\n",
    ") -> None:\n",
    "    target_suffix = (\n",
    "        target_format if target_format.startswith(\".\") else f\".{target_format}\"\n",
    "    )\n",
    "    target_file = file_path.with_suffix(target_suffix)\n",
    "\n",
    "    if check_exist and target_file.exists():\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        conversion_func(file_path, target_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting {file_path} to {target_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "@startthread\n",
    "def convert_files():\n",
    "    format_from = \"lbl\"\n",
    "    format_to = \"parquet\"\n",
    "    local_dir = Path.home() / \"data/juno\"\n",
    "    pattern = f\"**/*.{format_from}\"\n",
    "    convert_func = lbl2parquet\n",
    "    for file in local_dir.glob(pattern):\n",
    "        convert_file(file, format_to, convert_func)\n",
    "    logger.info(\"Done converting files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files with extension\n",
    "# find . -type f -name '*.parquet' -delete\n",
    "# find . -type f -name '*.orc' -delete\n",
    "# find . -type f -name '*.lbl' -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_pre_process(year, force=False):\n",
    "    trange = [f\"{year}-01-01\", f\"{year+1}-01-01T01\"]  # having some overlap\n",
    "    dir_path = Path.home() /  \"data/juno/JNO-SS-3-FGM-CAL-V1.0/\"\n",
    "    pattern = \"**/*.parquet\"\n",
    "    data = dir_path / pattern\n",
    "    \n",
    "    output = Path(f\"../data/jno_data_{year}.parquet\")\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(output) and not force:\n",
    "        logger.info(f\"File {output} exists. Skipping\")\n",
    "        return output\n",
    "    logger.info(f\"Preprocessing data for year {year}\")\n",
    "    \n",
    "    lazy_df = pl.scan_parquet(data)\n",
    "    temp_df = (\n",
    "        lazy_df.filter(\n",
    "            pl.col(\"time\").is_between(pd.Timestamp(trange[0]), pd.Timestamp(trange[1])),\n",
    "        )\n",
    "        .sort(\n",
    "            \"time\"\n",
    "        )  # needed for `compute_index_std` to work properly as `group_by_dynamic` requires the data to be sorted\n",
    "        .filter(\n",
    "            pl.col(\n",
    "                \"time\"\n",
    "            ).is_first_distinct()  # remove duplicate time values for xarray to select data properly, though significantly slows down the computation\n",
    "        )\n",
    "        .rename({\"BX SE\": \"BX\", \"BY SE\": \"BY\", \"BZ SE\": \"BZ\"})\n",
    "    )\n",
    "    temp_df.collect().write_parquet(output)\n",
    "    return output\n",
    "\n",
    "@startthread\n",
    "def batch_pre_process():\n",
    "    starting_year = starting_date.year\n",
    "    ending_year = ending_date.year\n",
    "\n",
    "    for year in range(starting_year, ending_year+1):\n",
    "        _batch_pre_process(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "def batch_process():\n",
    "    for year in tqdm(range(starting_date.year, ending_date.year+1)):\n",
    "        files = f'../data/{sat}_data_{year}.parquet'\n",
    "        output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "        \n",
    "        if os.path.exists(output):\n",
    "            logger.info(f\"Skipping {year} as the output file already exists.\")\n",
    "            continue\n",
    "\n",
    "        data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "        sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "\n",
    "        indices = compute_indices(data, tau)\n",
    "        # filter condition\n",
    "        sparse_num = tau / data_resolution // 3\n",
    "        filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "        candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "        candidates = convert_to_dataframe(candidates_pl)\n",
    "        \n",
    "        ids = process_candidates(candidates, sat_fgm, data, data_resolution)\n",
    "        ids.write_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "batch_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Combine all the files into one and remove duplicates\n",
    "#| eval: false\n",
    "files = f'../data/{sat}_candidates_*_tau_{tau.seconds}.parquet'\n",
    "output = f'../data/{sat}_candidates_tau_{tau.seconds}.parquet'\n",
    "ids = pl.scan_parquet(files).unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).collect()\n",
    "ids.write_parquet(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and read file from the server (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def juno_load_fgm(trange: list, coord=\"se\", data_rate=\"1s\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get the data array for a given time range and coordinate.\n",
    "\n",
    "    Parameters:\n",
    "        trange (list): The time range.\n",
    "        coord (str, optional): The coordinate. Defaults to 'se'.\n",
    "        data_rate (str, optional): The data rate. Defaults to '1s'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The dataframe for the given time range and coordinate.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(trange) != 2:\n",
    "        raise ValueError(\n",
    "            \"Expected trange to have exactly 2 elements: start and stop time.\"\n",
    "        )\n",
    "\n",
    "    start_time = pandas.Timestamp(trange[0])\n",
    "    stop_time = pandas.Timestamp(trange[1])\n",
    "\n",
    "    temp_index_df = index_df[\n",
    "        (index_df[\"SID\"] == get_sid(coord, data_rate))\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # Filtering\n",
    "    relevant_files = temp_index_df[\n",
    "        (temp_index_df[\"STOP_TIME\"] > start_time)\n",
    "        & (temp_index_df[\"START_TIME\"] < stop_time)\n",
    "    ]\n",
    "    dataframes = [download_and_read_lbl_file(row) for _, row in relevant_files.iterrows()]\n",
    "\n",
    "    # rows = [row for _, row in relevant_files.iterrows()]\n",
    "    # with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    #     dataframes = list(executor.map(download_and_read_file, rows))\n",
    "\n",
    "    combined_data = pandas.concat(dataframes)\n",
    "\n",
    "    return pdp_process_juno_df(combined_data)\n",
    "\n",
    "def get_sid(coord, data_rate):\n",
    "    sid_mapping = {\n",
    "        \"pc\": {\"1s\": \"PC 1 SECOND\", \"1min\": \"PC 1 MINUTE\", \"\": \"PCENTRIC\"},\n",
    "        \"pl\": {\"1s\": \"PAYLOAD 1 SECOND\", \"\": \"PAYLOAD\"},\n",
    "        \"ss\": {\"1s\": \"SS 1 SECOND\", \"1min\": \"SS 1 MINUTE\", \"\": \"SUNSTATE\"},\n",
    "        \"se\": {\"1s\": \"SE 1 SECOND\", \"1min\": \"SE 1 MINUTE\", \"\": \"SE\"},\n",
    "    }\n",
    "    try:\n",
    "        return sid_mapping[coord][data_rate]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "_skip_cond = ~pdp.cond.HasAllColumns([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"])\n",
    "pdp_process_juno_df = pdp.PdPipeline(\n",
    "    [\n",
    "        pdp.ColByFrameFunc(\n",
    "            \"time\",\n",
    "            lambda df: pandas.to_datetime(df[\"SAMPLE UTC\"], format=\"%Y %j %H %M %S %f\"),\n",
    "            skip=_skip_cond,\n",
    "        ),\n",
    "        pdp.ColDrop([\"SAMPLE UTC\", \"DECIMAL DAY\", \"INSTRUMENT RANGE\"], skip=_skip_cond),\n",
    "        pdp.df.set_index(\"time\"),\n",
    "        pdp.ColRename(col_renamer)\n",
    "        # pdp.AggByCols('SAMPLE UTC', func=lambda time: pandas.to_datetime(time, format='%Y %j %H %M %S %f'), func_desc='Convert time to datetime') # NOTE: this is quite slow\n",
    "        # pdp.df['time'] << pandas.to_datetime(pdp.df['SAMPLE UTC'], format='%Y %j %H %M %S %f'), # NOTE: this is not work\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
