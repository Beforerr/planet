{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Finding magnetic discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: import all the packages needed for the project\n",
    "#| export\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from ids_finder.utils import *\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import pytplot\n",
    "from pytplot import timebar, store_data, tplot, split_vec, join_vec, tplot_options, options, highlight, degap\n",
    "\n",
    "import pdpipe as pdp\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "from collections.abc import Callable\n",
    "from pandas import (\n",
    "    DataFrame,\n",
    "    Timestamp,\n",
    ")\n",
    "from xarray.core.dataarray import DataArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first index is $$ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} $$\n",
    "The second index is $$ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} $$\n",
    "The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\n",
    "\n",
    "third index (relative field jump) is $$ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} $$ a supplementary condition toreduce the uncertainty of recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_index_std in module ids_finder.utils:\n",
      "\n",
      "compute_index_std(data, tau)\n",
      "    helper function to compute standard deviation index\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(compute_index_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_index_fluctuation in module ids_finder.utils:\n",
      "\n",
      "compute_index_fluctuation(data, tau)\n",
      "    helper function to compute fluctuation index\n",
      "    \n",
      "    Notes: the results returned are a little bit different for the two implementations (because of the implementation of `std`).\n",
      "\n",
      "Help on function compute_index_fluctuation_xr in module ids_finder.utils:\n",
      "\n",
      "compute_index_fluctuation_xr(data: xarray.core.dataarray.DataArray, tau: int) -> xarray.core.dataarray.DataArray\n",
      "    Computes the fluctuation index for a given data array based on a specified time interval.\n",
      "    \n",
      "    Parameters:\n",
      "    - data: The xarray DataArray containing the data to be processed.\n",
      "    - tau: Time interval in seconds for resampling.\n",
      "    \n",
      "    Returns:\n",
      "    - fluctuation: xarray DataArray containing the fluctuation indices.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        ddof=0 is used for calculating the standard deviation. (ddof=1 is for sample standard deviation)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(compute_index_fluctuation)\n",
    "help(compute_index_fluctuation_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the relative field jump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dispatch(object, xr.DataArray)\n",
    "def get_candidate_data_xr(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    duration = candidate['tstop'] - candidate['tstart']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    return data.sel(time=slice(temp_tstart,  temp_tstop))\n",
    "\n",
    "@dispatch(object, pl.DataFrame)\n",
    "def get_candidate_data_pl(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    much slower than `get_candidate_data_xr`\n",
    "    \"\"\"\n",
    "    duration = candidate['tstart'] - candidate['tstop']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    temp_data = data.filter(\n",
    "        pl.col(\"time\").is_between(temp_tstart, temp_tstop)\n",
    "    )\n",
    "    \n",
    "    dims = [\"v_dim\", \"time\"]\n",
    "    coords = {\n",
    "        \"time\": temp_data['time'], \n",
    "        \"v_dim\": [\"BX\", \"BY\", \"BZ\"]\n",
    "        }\n",
    "    return xr.DataArray([ temp_data['BX'], temp_data['BY'], temp_data['BZ']], dims=dims, coords=coords)\n",
    "\n",
    "def get_candidate_data(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        return get_candidate_data_xr(candidate, data, coord=coord, neighbor=neighbor)\n",
    "    elif isinstance(data, pl.DataFrame):    \n",
    "        return get_candidate_data_pl(candidate, data, coord=coord, neighbor=neighbor)\n",
    "\n",
    "def get_candidates(candidates: DataFrame, candidate_type=None, num:int=4):\n",
    "    \n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates['type'] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "    \n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\")\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyspedas.cotrans import minvar_matrix_make\n",
    "from pyspedas import tvector_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_basic(\n",
    "    data, tstart, tstop, tau, mva_tstart=None, mva_tstop=None, neighbor: int = 1\n",
    "):\n",
    "    if mva_tstart is None:\n",
    "        mva_tstart = tstart\n",
    "    if mva_tstop is None:\n",
    "        mva_tstop = tstop\n",
    "\n",
    "    mva_b = data.sel(time=slice(mva_tstart, mva_tstop))\n",
    "    store_data(\"fgm\", data={\"x\": mva_b.time, \"y\": mva_b})\n",
    "    minvar_matrix_make(\"fgm\")  # get the MVA matrix\n",
    "\n",
    "    temp_tstart = pd.Timestamp(tstart) - pd.Timedelta(neighbor * tau, unit=\"s\")\n",
    "    temp_tstop = pd.Timestamp(tstop) + pd.Timedelta(neighbor * tau, unit=\"s\")\n",
    "\n",
    "    temp_b = data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "    store_data(\"fgm\", data={\"x\": temp_b.time, \"y\": temp_b})\n",
    "    temp_btotal = calc_vec_mag(temp_b)\n",
    "    store_data(\"fgm_btotal\", data={\"x\": temp_btotal.time, \"y\": temp_btotal})\n",
    "\n",
    "    tvector_rotate(\"fgm_mva_mat\", \"fgm\")\n",
    "    split_vec(\"fgm_rot\")\n",
    "    pytplot.data_quants[\"fgm_btotal\"][\"time\"] = pytplot.data_quants[\"fgm_rot\"][\n",
    "        \"time\"\n",
    "    ]  # NOTE: whenever using `get_data`, we may lose precision in the time values. This is a workaround.\n",
    "    join_vec(\n",
    "        [\n",
    "            \"fgm_rot_x\",\n",
    "            \"fgm_rot_y\",\n",
    "            \"fgm_rot_z\",\n",
    "            \"fgm_btotal\",\n",
    "        ],\n",
    "        new_tvar=\"fgm_all\",\n",
    "    )\n",
    "\n",
    "    options(\"fgm\", \"legend_names\", [r\"$B_x$\", r\"$B_y$\", r\"$B_z$\"])\n",
    "    options(\"fgm_all\", \"legend_names\", [r\"$B_l$\", r\"$B_m$\", r\"$B_n$\", r\"$B_{total}$\"])\n",
    "    options(\"fgm_all\", \"ysubtitle\", \"[nT LMN]\")\n",
    "    highlight([\"fgm\", \"fgm_all\"], [tstart.timestamp(), tstop.timestamp()])\n",
    "    degap(\"fgm\")\n",
    "    degap(\"fgm_all\")\n",
    "\n",
    "def format_candidate_title(candidate: pandas.Series):\n",
    "    format_float = lambda x: rf\"$\\bf {x:.2f} $\" if isinstance(x, (float, int)) else rf\"$\\bf {x} $\"\n",
    "\n",
    "    base_line = rf'$\\bf {candidate.get(\"type\", \"N/A\")} $ candidate (time: {candidate.get(\"time\", \"N/A\")}) with index '\n",
    "    index_line = rf'i1: {format_float(candidate.get(\"index_std\", \"N/A\"))}, i2: {format_float(candidate.get(\"index_fluctuation\", \"N/A\"))}, i3: {format_float(candidate.get(\"index_diff\", \"N/A\"))}'\n",
    "    info_line = rf'$B_n/B$: {format_float(candidate.get(\"BnOverB\", \"N/A\"))}, $dB/B$: {format_float(candidate.get(\"dBOverB\", \"N/A\"))}, $(dB/B)_{{max}}$: {format_float(candidate.get(\"dBOverB_max\", \"N/A\"))},  $Q_{{mva}}$: {format_float(candidate.get(\"Q_mva\", \"N/A\"))}'\n",
    "    title = rf\"\"\"{base_line}\n",
    "    {index_line}\n",
    "    {info_line}\"\"\"\n",
    "    return title\n",
    "\n",
    "\n",
    "def plot_candidate(candidate: pandas.Series):\n",
    "    if pd.notnull(candidate.get(\"d_tstart\")) and pd.notnull(candidate.get(\"d_tstop\")):\n",
    "        plot_basic(\n",
    "            sat_fgm,\n",
    "            candidate[\"tstart\"],\n",
    "            candidate[\"tstop\"],\n",
    "            tau,\n",
    "            candidate[\"d_tstart\"],\n",
    "            candidate[\"d_tstop\"],\n",
    "        )\n",
    "    else:\n",
    "        plot_basic(sat_fgm, candidate[\"tstart\"], candidate[\"tstop\"], tau)\n",
    "\n",
    "    tplot_options(\"title\", format_candidate_title(candidate))\n",
    "\n",
    "    if \"d_time\" in candidate.keys():\n",
    "        timebar(candidate[\"d_time\"].timestamp(), color=\"red\")\n",
    "    if \"d_tstart\" in candidate.keys() and not pd.isnull(candidate[\"d_tstart\"]):\n",
    "        timebar(candidate[\"d_tstart\"].timestamp())\n",
    "    if \"d_tstop\" in candidate.keys() and not pd.isnull(candidate[\"d_tstop\"]):\n",
    "        timebar(candidate[\"d_tstop\"].timestamp())\n",
    "\n",
    "    # tplot(['fgm','fgm_all'])\n",
    "    tplot(\"fgm_all\")\n",
    "\n",
    "\n",
    "def plot_candidates(\n",
    "    candidates: pandas.DataFrame, candidate_type=None, num=4, plot_func=plot_candidate\n",
    "):\n",
    "    \"\"\"Plot a set of candidates.\n",
    "\n",
    "    Parameters:\n",
    "    - candidates (pd.DataFrame): DataFrame containing the candidates.\n",
    "    - candidate_type (str, optional): Filter candidates based on a specific type.\n",
    "    - num (int): Number of candidates to plot, selected randomly.\n",
    "    - plot_func (callable): Function used to plot an individual candidate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter by candidate_type if provided\n",
    "    candidates = get_candidates(candidates, candidate_type, num)\n",
    "\n",
    "    # Plot each candidate using the provided plotting function\n",
    "    for _, candidate in candidates.iterrows():\n",
    "        plot_func(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of duration\n",
    "- Define $d^* = \\max( | dB / dt | ) $, and then define time interval where $| dB/dt |$ decreases to $d^*/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "THRESHOLD_RATIO  = 1/4\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def calc_duration(vec: xr.DataArray, threshold_ratio=THRESHOLD_RATIO) -> pandas.Series:\n",
    "    # NOTE: gradient calculated at the edge is not reliable.\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\").isel(time=slice(1,-1))\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    # Determine d_star based on trend\n",
    "    if vec_diff_mag.isnull().all():\n",
    "        raise ValueError(\"The differentiated vector magnitude contains only NaN values. Cannot compute duration.\")\n",
    "    \n",
    "    d_star_index = vec_diff_mag.argmax(dim=\"time\")\n",
    "    d_star = vec_diff_mag[d_star_index]\n",
    "    d_time = vec_diff_mag.time[d_star_index]\n",
    "    \n",
    "    threshold = d_star * threshold_ratio\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_star': d_star.item(),\n",
    "        'd_time': d_time.values,\n",
    "        'threshold': threshold.item(),\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    "\n",
    "def calc_d_duration(vec: xr.DataArray, d_time, threshold) -> pd.Series:\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    " \n",
    "def find_start_end_times(vec_diff_mag: xr.DataArray, d_time, threshold) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    # Determine start time\n",
    "    pre_vec_mag = vec_diff_mag.sel(time=slice(None, d_time))\n",
    "    start_time = get_time_from_condition(pre_vec_mag, threshold, \"last_below\")\n",
    "\n",
    "    # Determine stop time\n",
    "    post_vec_mag = vec_diff_mag.sel(time=slice(d_time, None))\n",
    "    end_time = get_time_from_condition(post_vec_mag, threshold, \"first_below\")\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def get_time_from_condition(vec: xr.DataArray, threshold, condition_type) -> pd.Timestamp:\n",
    "    if condition_type == \"first_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = 0\n",
    "    elif condition_type == \"last_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition_type: {condition_type}\")\n",
    "\n",
    "    where_result = np.where(condition)[0]\n",
    "\n",
    "    if len(where_result) > 0:\n",
    "        return vec.time[where_result[index_choice]].values\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_duration(candidate: pd.Series, data, get_candidate_data_fn:Callable =get_candidate_data_xr) -> pd.Series:\n",
    "    try:\n",
    "        candidate_data = get_candidate_data_fn(candidate, data)\n",
    "        return calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calc_candidate_d_duration(candidate, data , get_candidate_data_fn:Callable =get_candidate_data) -> pd.Series:\n",
    "    try:\n",
    "        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n",
    "            candidate_data = get_candidate_data_fn(candidate, data, neighbor=1)\n",
    "            d_time = candidate['d_time']\n",
    "            threshold = candidate['threshold']\n",
    "            return calc_d_duration(candidate_data, d_time, threshold)\n",
    "        else:\n",
    "            return pandas.Series({\n",
    "                'd_tstart': candidate['d_tstart'],\n",
    "                'd_tstop': candidate['d_tstop'],\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calibrate_candidate_duration(\n",
    "    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibrates the candidate duration. \n",
    "    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n",
    "    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - pd.Series: The calibrated candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_notnull = pd.notnull(candidate['d_tstart'])\n",
    "    stop_notnull = pd.notnull(candidate['d_tstop']) \n",
    "    \n",
    "    match start_notnull, stop_notnull:\n",
    "        case (True, True):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (True, False):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n",
    "        case (False, True):\n",
    "            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (False, False):\n",
    "            return pandas.Series({\n",
    "                'd_tstart': None,\n",
    "                'd_tstop': None,\n",
    "            })\n",
    "    \n",
    "    duration = d_tstop - d_tstart\n",
    "    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n",
    "    \n",
    "    if num_of_points_between <= (duration/data_resolution) * ratio:\n",
    "        d_tstart = None\n",
    "        d_tstop = None\n",
    "    \n",
    "    return pandas.Series({\n",
    "        'd_tstart': d_tstart,\n",
    "        'd_tstop': d_tstop,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID classification\n",
    "\n",
    "In this method, TDs and RDs satisfy $ \\frac{ |B_N| }{ |B_{bg}| } < 0.2$ and $ | \\frac{ \\Delta |B| }{ |B_{bg}| } | > 0.4$ B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with < 0.4 B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as > 0.4 B BN bg ∣∣ ∣∣ , > D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\n",
    "\n",
    "\n",
    "Criteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\n",
    "\n",
    "| Type   |  $\\|B_n/B\\|$      | $\\| \\Delta B / B \\|$  |\n",
    "|----------|-------------|------|\n",
    "| Rotational (RD) | large | small |\n",
    "| Tangential (TD) | small |  large |\n",
    "| Either (ED) | small | small |\n",
    "| Neither (ND) | large | large |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minimum variance analysis (MVA)\n",
    "\n",
    "To ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "BnOverB_RD_lower_threshold = 0.4\n",
    "dBOverB_RD_upper_threshold = 0.2\n",
    "\n",
    "BnOverB_TD_upper_threshold = 0.2\n",
    "dBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n",
    "\n",
    "BnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\n",
    "dBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n",
    "\n",
    "BnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\n",
    "dBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyspedas.cotrans.minvar import minvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_classification_index(data: xr.DataArray):\n",
    "    \n",
    "    vrot, v, w = minvar(data.to_numpy()) # NOTE: using `.to_numpy()` will significantly speed up the computation.\n",
    "    Vl = v[:,0] # Maximum variance direction eigenvector\n",
    "\n",
    "    B_rot = xr.DataArray(vrot, dims=['time', 'v_dim'], coords={'time': data.time})\n",
    "    B = calc_vec_mag(B_rot)\n",
    "    B_n = B_rot.isel(v_dim=2)\n",
    "    \n",
    "    B_mean = B.mean(dim=\"time\")\n",
    "    B_n_mean = B_n.mean(dim=\"time\")\n",
    "    \n",
    "    BnOverB = B_n_mean / B_mean\n",
    "    # BnOverB = np.abs(B_n / B).mean(dim=\"time\")\n",
    "\n",
    "    dB = B.isel(time=-1) - B.isel(time=0)\n",
    "    dBOverB = np.abs(dB / B_mean)\n",
    "    dBOverB_max = (B.max(dim=\"time\") - B.min(dim=\"time\")) / B_mean\n",
    "    \n",
    "    \n",
    "    return pandas.Series({\n",
    "        'Vl_x': Vl[0],\n",
    "        'Vl_y': Vl[1],\n",
    "        'Vl_z': Vl[2],\n",
    "        'eig0': w[0],\n",
    "        'eig1': w[1],\n",
    "        'eig2': w[2],\n",
    "        'Q_mva': w[1]/w[2],\n",
    "        'B': B_mean.item(),\n",
    "        'B_n': B_n_mean.item(),\n",
    "        'dB': dB.item(),\n",
    "        'BnOverB': BnOverB.item(), \n",
    "        'dBOverB': dBOverB.item(),\n",
    "        'dBOverB_max': dBOverB_max.item(),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def classify_id(BnOverB, dBOverB):\n",
    "    BnOverB = np.abs(np.asarray(BnOverB))\n",
    "    dBOverB = np.asarray(dBOverB)\n",
    "\n",
    "    s1 = (BnOverB > BnOverB_RD_lower_threshold)\n",
    "    s2 = (dBOverB > dBOverB_RD_upper_threshold)\n",
    "    s3 = (BnOverB > BnOverB_TD_upper_threshold)\n",
    "    s4 = s2 # note: s4 = (dBOverB > dBOverB_TD_lower_threshold)\n",
    "    \n",
    "    RD = s1 & ~s2\n",
    "    TD = ~s3 & s4\n",
    "    ED = ~s1 & ~s4\n",
    "    ND = s3 & s2\n",
    "\n",
    "    # Create an empty result array with the same shape\n",
    "    result = np.empty_like(BnOverB, dtype=object)\n",
    "\n",
    "    result[RD] = \"RD\"\n",
    "    result[TD] = \"TD\"\n",
    "    result[ED] = \"ED\"\n",
    "    result[ND] = \"ND\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector.\n",
    "    - v2: The second vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # convert xr.Dataarray to numpy arrays\n",
    "    if isinstance(v1, DataArray):\n",
    "        v1 = v1.to_numpy()\n",
    "    if isinstance(v2, DataArray):\n",
    "        v2 = v2.to_numpy()\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calc_candidate_rotation_angle(candidates, data:  xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = candidates['d_tstart']\n",
    "    tstop = candidates['d_tstop']\n",
    "    \n",
    "    # Convert Series to numpy arrays if necessary\n",
    "    if isinstance(tstart, pd.Series):\n",
    "        tstart = tstart.to_numpy()\n",
    "        tstop = tstop.to_numpy()\n",
    "        # no need to Handle NaT values (as `calibrate_candidate_duration` will handle this)\n",
    "    \n",
    "    # Get the vectors at the two time steps\n",
    "    vecs_before = data.sel(time=tstart, method=\"nearest\")\n",
    "    vecs_after = data.sel(time=tstop, method=\"nearest\")\n",
    "    \n",
    "    # Compute the rotation angle(s)\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign satellite locations to the discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_candidate_location(candidate, location_data: DataArray):\n",
    "    return location_data.sel(time = candidate['d_time']).to_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_ID_filter_condition(\n",
    "    index_std_threshold = 2,\n",
    "    index_fluc_threshold = 1,\n",
    "    index_diff_threshold = 0.1,\n",
    "    sparse_num = 15\n",
    "):\n",
    "    return (\n",
    "        (pl.col(\"index_std\") > index_std_threshold)\n",
    "        & (pl.col(\"index_fluctuation\") > index_fluc_threshold)\n",
    "        & (pl.col(\"index_diff\") > index_diff_threshold)\n",
    "        & (\n",
    "            pl.col(\"index_std\").is_finite()\n",
    "        )  # for cases where neighboring groups have std=0\n",
    "        & (\n",
    "            pl.col(\"count\") > sparse_num\n",
    "        )  # filter out sparse intervals, which may give unreasonable results.\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pdpipe.util import out_of_place_col_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: patch `pdp.ApplyToRows` to work with `modin` DataFrames\n",
    "#| export\n",
    "\n",
    "@patch\n",
    "def _transform(self: pdp.ApplyToRows, X, verbose):\n",
    "    new_cols = X.apply(self._func, axis=1)\n",
    "    if isinstance(new_cols, (pd.Series, pandas.Series)):\n",
    "        loc = len(X.columns)\n",
    "        if self._follow_column:\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "        return out_of_place_col_insert(\n",
    "            X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "        )\n",
    "    if isinstance(new_cols, (pd.DataFrame, pandas.DataFrame)):\n",
    "        sorted_cols = sorted(list(new_cols.columns))\n",
    "        new_cols = new_cols[sorted_cols]\n",
    "        if self._follow_column:\n",
    "            inter_X = X\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            for colname in new_cols.columns:\n",
    "                inter_X = out_of_place_col_insert(\n",
    "                    X=inter_X,\n",
    "                    series=new_cols[colname],\n",
    "                    loc=loc,\n",
    "                    column_name=colname,\n",
    "                )\n",
    "                loc += 1\n",
    "            return inter_X\n",
    "        assign_map = {\n",
    "            colname: new_cols[colname] for colname in new_cols.columns\n",
    "        }\n",
    "        return X.assign(**assign_map)\n",
    "    raise TypeError(  # pragma: no cover\n",
    "        \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "        \" Only Series and DataFrame are allowed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_classification_index(candidate, data):\n",
    "    return calc_classification_index(\n",
    "        data.sel(time=slice(candidate[\"d_tstart\"], candidate[\"d_tstop\"]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_to_dataframe(\n",
    "    data: pl.DataFrame, # orignal Dataframe\n",
    "):\n",
    "    \"convert data into a pandas/modin DataFrame\"\n",
    "    if isinstance(data, pl.LazyFrame):\n",
    "        data = data.collect().to_pandas(use_pyarrow_extension_array=True)\n",
    "    if isinstance(data, pl.DataFrame):\n",
    "        data = data.to_pandas(use_pyarrow_extension_array=True)\n",
    "    if not isinstance(data, pd.DataFrame):  # `modin` supports\n",
    "        data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Pipelines Class for processing IDs\n",
    "#| export\n",
    "class IDsPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # fmt: off\n",
    "    def calc_duration(self, sat_fgm: xr.DataArray):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters\"\n",
    "            ),\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_d_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters if needed\"\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def calibrate_duration(self, sat_fgm, data_resolution):\n",
    "        return \\\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calibrate_candidate_duration(candidate, sat_fgm, data_resolution),\n",
    "                func_desc=\"calibrating duration parameters if needed\"\n",
    "            )\n",
    "\n",
    "    def classify_id(self, sat_fgm):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_classification_index(candidate, sat_fgm),\n",
    "                func_desc='calculating index \"q_mva\", \"BnOverB\" and \"dBOverB\"'\n",
    "            ),\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"type\",\n",
    "                lambda df: classify_id(df[\"BnOverB\"], df[\"dBOverB\"]),\n",
    "                func_desc=\"classifying the type of the ID\"\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def calc_rotation_angle(self, sat_fgm):\n",
    "        return \\\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"rotation_angle\",\n",
    "                lambda df: calc_candidate_rotation_angle(df, sat_fgm),\n",
    "                func_desc=\"calculating rotation angle\",\n",
    "            ) \n",
    "\n",
    "    def assign_coordinates(self, sat_state):\n",
    "        return \\\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: get_candidate_location(candidate, sat_state),\n",
    "                func_desc=\"assigning coordinates\",\n",
    "            )\n",
    "    # fmt: on\n",
    "    # ... you can add more methods as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_candidates(\n",
    "    candidates: pl.DataFrame, \n",
    "    sat_fgm: xr.DataArray, \n",
    "    sat_state: xr.DataArray,\n",
    "    data_resolution: timedelta\n",
    "):\n",
    "    \n",
    "    id_pipelines = IDsPipeline()\n",
    "\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    # calibrate duration\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]\n",
    "    if not temp_candidates.empty:\n",
    "        candidates.update(\n",
    "            id_pipelines.calibrate_duration(sat_fgm, data_resolution).apply(temp_candidates)\n",
    "        )\n",
    "\n",
    "    candidates = candidates.dropna()  # Remove candidates with NaN values\n",
    "    \n",
    "    ids = (\n",
    "        id_pipelines.classify_id(sat_fgm) + \n",
    "        id_pipelines.calc_rotation_angle(sat_fgm) +\n",
    "        id_pipelines.assign_coordinates(sat_state)\n",
    "    ).apply(candidates)\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Sep-23 22:58:43: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Estimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n"
     ]
    }
   ],
   "source": [
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2011\n",
    "    files = f'../data/{sat}_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "    sat_fgm = df2ts(data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "    sat_state = df2ts(data, [\"X\", \"Y\", \"Z\"], attrs={\"coordinate_system\": coord, \"units\": \"km\"})\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "    \n",
    "    ids = process_candidates(candidates, sat_fgm, sat_state, data_resolution)\n",
    "    df = pandas.DataFrame(ids)\n",
    "    # pandas.DataFrame(ids).to_parquet(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Sep-23 23:06:55: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Estimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n"
     ]
    }
   ],
   "source": [
    "# Test different libraries to parallelize the computation\n",
    "test = True\n",
    "if test:\n",
    "    pdp_test = pdp.ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, jno_fgm)),\n",
    "        # lambda candidate: calc_duration(jno_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    candidates_pd = candidates.to_pandas()\n",
    "    candidates_modin = pd.DataFrame(candidates_pd)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # Standard case: 37+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=jno_fgm) # this works with dask, 60 secs\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works, 8 secs # not work? `DataFrame' object has no attribute 'mapply'\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, jno_fgm), axis=1) # this works with ray, 8 secs # NOTE: can not work with dask\n",
    "    pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=jno_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n",
    "    # pdp_test(candidates_modin) # TypeError: Unexpected type generated by applying a function to a DataFrame. Only Series and DataFrame are allowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_candidates(candidates.loc[lambda _: _['time']=='2012-07-10 02:31:15'])\n",
    "temp_trange = ['2012-07-15 03:44', '2012-07-15 03:47']\n",
    "temp_data = sat_fgm.sel(time=slice(*temp_trange))\n",
    "temp_data.plot.scatter(x='time', hue='v_dim')\n",
    "# temp_data.resample(time=pd.Timedelta(tau, unit='s')).map(calc_vec_std)\n",
    "compute_index_std(temp_data, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: neighboring data is missing, causing the calculation of the standard deviation index to be Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case: neighboring data is missing, causing the calculation of the standard deviation index to be Inf.\n",
    "temp_trange = ['2012-07-10 02:30', '2012-07-10 02:32']\n",
    "temp_data = sat_fgm.sel(time=slice(*temp_trange))\n",
    "temp_data.plot.scatter(x='time', hue='v_dim')\n",
    "# temp_data.resample(time=pd.Timedelta(tau, unit='s')).map(calc_vec_std)\n",
    "compute_index_std(temp_data, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candidates(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Not very accurate for waving magnetic field..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_candidate = {'time': Timestamp('2012-05-01 00:39:12'),\n",
    " 'tstart': Timestamp('2012-05-01 00:38:56'),\n",
    " 'tstop': Timestamp('2012-05-01 00:39:28'),\n",
    " 'i1': 2.891042053414383,\n",
    " 'i2': 2.389699609352786,\n",
    " 'i3': 1.3916002784658887,\n",
    " 'd_star': 0.27143595,\n",
    " 'd_time': Timestamp('2012-05-01 00:39:18.672000'),\n",
    " 'd_tstart': Timestamp('2012-05-01 00:39:14.672000'),\n",
    " 'd_tstop': Timestamp('2012-05-01 00:39:19.671000'),\n",
    "}\n",
    "\n",
    "plot_candidate(temp_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_candidate_data_xr(temp_candidate, neighbor=1)\n",
    "vec_diff = data.differentiate(\"time\", datetime_unit=\"s\", edge_order=2).isel(time=slice(1,-1))\n",
    "vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "vec_diff_mag.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Small threshold_ratio values will tend to make the duration longer if the duration can be determined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different threshold ratios\n",
    "threshold_ratios = [1/8, 1/4, 0.3, 1/3, 1/2]\n",
    "for threshold_ratio in threshold_ratios:\n",
    "    temp_candidate.update(calc_duration(get_candidate_data_xr(temp_candidate), threshold_ratio=threshold_ratio).to_dict())\n",
    "    plot_candidate(temp_candidate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_solar_wind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
